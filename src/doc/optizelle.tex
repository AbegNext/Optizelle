\documentclass{report}

% All of the packages required

% AMS math typesetting
\usepackage{amssymb}
\usepackage{amsmath}

% Grabs code examples
\usepackage{listings}

% We use this with listings in order to color our code
\usepackage{xcolor}

% Creates both internal and external links.  At the moment, I like this
% more than the url package
\usepackage[colorlinks=true]{hyperref}

% Creates multiple page tables
\usepackage{longtable}

% Sets both the paper type (letter or A4) as well as the margins
\usepackage[top=0.1\paperheight,bottom=0.1\paperheight,left=0.1\paperwidth,right=0.1\paperwidth,paper=@PAPERTYPE@]{geometry}

% Creates a slightly nicer looking manual.  Make sure this comes after geometry
% or else all of the styles will be messed up.
\usepackage[Bjornstrup]{fncychap} 

% Type sets directory trees
\usepackage{dirtree}

% Allows the creation of starred macros
\usepackage{suffix}

% Allows the external creation of files.  We use this in order to get around
% some of the irritating change in formatting that hyperref does.
\usepackage{filecontents}

% Define a computer text environment.  This is monospaced and allows
% underscores.  Code found at:
% https://tex.stackexchange.com/questions/20890/define-an-escape-underscore-environment
\makeatletter
\DeclareRobustCommand*{\textct}[1]{%
  \begingroup\@activeus\scantokens{\texttt{#1}\endinput}\endgroup}
\begingroup\lccode`\~=`\_\relax
   \lowercase{\endgroup\def\@activeus{\catcode`\_=\active \let~\_}}
\makeatother

% Used for labeling and referring to computer text 
\newcommand{\textctlabel}[1]{\phantomsection\label{itm:#1}\textct{#1}}
\newcommand{\textctref}[1]{\hyperref[itm:#1]{\textct{#1}}}

% Setup a CMake list environment.
\newenvironment{boldlist}
    {\begin{list}{}{
        \labelwidth.15\textwidth
        \leftmargin\dimexpr\leftmargin+.15\textwidth
        \renewcommand\makelabel[1]{%
            \textbf{##1}}}}
    {\vspace{-\dimexpr\baselineskip+2\itemsep}\end{list}}

% CMake items
\newcommand{\cmakeitem}[7]{
    \item[Flag] #1
    \item[Type]  #2
    \item[Default] #3
    \item[Dependency] #4
    \item[Enables] #5
    \item[Autodetect?] #6
    \item[Description] #7
    \item[]}

%# API items
\newcommand{\apiitem}[4]{
    \item[Language] #1
    \item[Structure]  #2
    \item[Interface] #3
    \item[Code] #4
    \item[]}

\newcommand{\apiitemshort}[3]{
    \item[Language] #1
    \item[Structure]  #2
    \item[Interface] #3
    \item[]}

% Describing the vector space
\newcommand{\vsitem}[4]{
    \item[Language] #1
    \item[Vector]  #2
    \item[Operations]  #3
    \item[Interface] #4
    \item[]}

% Example items 
\newcommand{\exampleitem}[2]{
    \item[Language] #1
    \item[Code] #2
    \item[]}

% Function bundle items
\newcommand{\functionitem}[5]{
    \item[Element] #1
    \item[Type] #2
    \item[\mbox{Problem Class}] #3
    \item[Required] #4
    \item[Description] #5
    \item[]}


% Change the figure numbering to be chap.sec.num
\renewcommand{\thefigure}{\arabic{chapter}.\arabic{section}.\arabic{figure}}
        
% Create some new environments for displaying code and output

% Use the following for all languages
\lstset{
    boxpos=t,
    showstringspaces=false,
    basicstyle=\footnotesize,
    backgroundcolor=\color{white},
    includerangemarker=false,
    keywordstyle=\color{blue}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    commentstyle=\color{green!40!black}\ttfamily,
    morecomment=[l][\color{magenta}]{\#}
}

% C++
\lstdefinestyle{C++}{
    language=C++,
    rangeprefix=//---,
    rangesuffix=---
}

% Python
\lstdefinestyle{Python}{
    language=Python,
    rangeprefix=\#---,
    rangesuffix=---,
}

% MATLAB
\lstdefinestyle{MATLAB}{
    language=MATLAB,
    rangeprefix=\%---,
    rangesuffix=---,
}

% JSON 
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}
\lstdefinestyle{json}{
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{white},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {e-}{{{\color{numb}e-}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
    rangeprefix=\"---,
    rangesuffix=---\"\ :\ null
}

% Optizelle output
\lstdefinestyle{OptizelleOutput}{
    morekeywords={Iter,f,x,merit,grad,dx,KryWhy,KryErr,KryIter,ared,pred,
        ared/pred,mu,mu_est,g},
    sensitive=false,
    literate=
}

% Simple macros to help math printing
\newcommand{\re}{\mathbb{R}}
\newcommand{\st}{\textnormal{st}}
\newcommand{\ineqGradLag}[2]{\nabla f(#1)-h^\prime(#1)^*#2}
\newcommand{\eqGradLag}[2]{\nabla f(#1)+g^\prime(#1)^*#2}
\newcommand{\conGradLag}[3]{\nabla f(#1)+g^\prime(#1)^*#2-h^\prime(#1)^*#3}

% Collections of text, tables, etc.
\newsavebox{\boxOptimizationTypes}
\newsavebox{\boxUnconstrained}
\newsavebox{\boxEqualityConstrained}
\newsavebox{\boxInequalityConstrained}
\newsavebox{\boxConstrained}
\newsavebox{\boxImport}

% Labels for some sections
\newcommand{\secimport}{Import Optizelle}
\newcommand{\secimportvs}{Import or define the appropriate vector spaces}
\newcommand{\secobjective}{Define the objective function}
\newcommand{\secconstraints}{(Optional) Define the constraints}
\newcommand{\secpreconditioners}{(Optional) Define the preconditioners}
\newcommand{\secstate}{Create the optimization state}
\newcommand{\secparams}{Set the optimization parameters}
\newcommand{\secaccumulatefns}{Accumulate the functions}
\newcommand{\secsolve}{Call the optimization solver}
\newcommand{\secextract}{Extract the solution}
\newcommand{\seccompilerun}{Compile/run the program}

\newcommand{\chparameters}{Optimization parameters}
\newcommand{\seccustomvector}{Vector spaces}
\newcommand{\secmessaging}{Messaging objects}

\title{Optizelle v@CPACK_PACKAGE_VERSION_MAJOR@.@CPACK_PACKAGE_VERSION_MINOR@.@CPACK_PACKAGE_VERSION_PATCH@}
\author{OptimoJoe\\Joseph Young}
\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction}

        Optizelle [op-t\textit{\!uh}-{\bf zel}] is an open source software library designed to solve general purpose nonlinear optimization problems of the form
    \begin{lrbox}{\boxUnconstrained}
        $\begin{array}[t]{rcl}
            \min\limits_{x\in X} && f(x)
        \end{array}$
    \end{lrbox}
    \begin{lrbox}{\boxEqualityConstrained}
        $\begin{array}[t]{rcl}
            \min\limits_{x\in X} && f(x)\\
            \st && g(x)=0
        \end{array}$
    \end{lrbox}
    \begin{lrbox}{\boxInequalityConstrained}
        $\begin{array}[t]{rcl}
            \min\limits_{x\in X} && f(x)\\
            \st && h(x)\succeq 0
        \end{array}$
    \end{lrbox}
    \begin{lrbox}{\boxConstrained}
        $\begin{array}[t]{rcl}
            \min\limits_{x\in X} && f(x)\\
            \st && g(x)=0\\
                && h(x)\succeq 0
        \end{array}$
    \end{lrbox}
    \begin{lrbox}{\boxOptimizationTypes}
        \begin{tabular}{|l|l|}\hline
            \multicolumn{1}{|c|}{\bf Unconstrained} &
            \multicolumn{1}{c|}{\bf Equality Constrained}\\
            \usebox{\boxUnconstrained} &
            \usebox{\boxEqualityConstrained} \\\hline
            \multicolumn{1}{|c|}{\bf Inequality Constrained} &
            \multicolumn{1}{c|}{\bf Constrained}\\
            \usebox{\boxInequalityConstrained} &
            \usebox{\boxConstrained} \\\hline
        \end{tabular}
    \end{lrbox}
\begin{center}
    \usebox{\boxOptimizationTypes}
\end{center}
\noindent It features 
\begin{itemize}
    \item {\bf State of the art algorithms}
        \begin{itemize}
            \item Unconstrained -- Steepest Descent, Preconditioned Nonlinear-CG (Fletcher-Reeves, Polak-Ribiere, Hestenes-Stiefel), BFGS, Newton-CG, SR1, Trust-Region Newton (Truncated-CG and Truncated-MINRES), Barzilai-Borwein Two-Point Approximation.
            \item Equality Constrained -- Inexact Composite Step SQP.
            \item Inequality Constrained -- Primal-Dual Interior Point Method for Cone Constraints (Linear, Second-Order Cone, and Semidefinite), Log-Barrier Method for Cone Constraints.
            \item Constrained -- Any combination of the above.
        \end{itemize}
    \item {\bf Open source}
        \begin{itemize}
            \item Released under the 2-Clause BSD License.
            \item Free and ready to use with both open and closed sourced commericial codes.
        \end{itemize}
    \item {\bf Multilanguage support}
        \begin{itemize}
            \item Interfaces to C++, MATLAB/Octave, and Python.
        \end{itemize}
    \item {\bf Robust computations and repeatibility}
        \begin{itemize}
            \item Can stop, archive, and restart the computation from any optimization iteration.
            \item Combined with the multilanguage support, the optimization can be started in one language and migrated to another.  For example, archived optimization runs that started in MATLAB can be migrated and completed in C++.
        \end{itemize}
    \item {\bf User-defined parallelism}
        \begin{itemize}
            \item Fully compatible with OpenMP, MPI, or GPUs.
        \end{itemize}
    \item {\bf Extensible linear algebra}
        \begin{itemize}
            \item Supports user-defined vector algebra and preconditioners.
            \item Enables sparse, dense, and matrix-free computations.
            \item Ability to define custom inner products and compatibility with preconditioners such as algebraic multigrid make Optizelle well-suited for PDE constrained optimization.
        \end{itemize}
    \item {\bf Sophisticated Control of the Optimization Algorithms}
        \begin{itemize}
            \item Allows the user to insert arbitrary code into the optimization algorithm, which enables custom heuristics to be embedded without modifying the source.  For example, in signal processing applications, the optimization iterates could be run through a band-pass filter at the end of each optimization iteration.
        \end{itemize}
\end{itemize}

\section{Licensing}

        Optizelle is copyrighted by OptimoJoe and licensed under the 2-Clause BSD License:
\lstinputlisting{@LICENSEPATH@/LICENSE}
In short, Optizelle is free to use in both open and closed sourced codes.  If you do so, we ask that you provide a citation or link to \url{http://www.optimojoe.com}.

\section{Support}
        
        News, updates, download information, and support forums for Optizelle can be found at
\begin{center}
    \mbox{\url{http://www.optimojoe.com/optizelle}}
\end{center}
If you are interested in paid support and consulting, please contact us at \mbox{\href{mailto:contact@optimojoe.com}{contact@optimojoe.com}}.

\section{Brief Example}

        In order to see a short example of Optizelle in action, consider the unconstrained minimization of the Rosenbrock function
$$
    \begin{array}{rcl}
        \min\limits_{x\in\re^2} && (1-x_1)^2+100(x_2-x_1^2)^2.
    \end{array}
$$
In order to optimize this function, we use the following code and parameters, which generates the subsequent output.
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=MATLAB]{@ROSENBROCKPATH@/rosenbrock.m}}

    \exampleitem
        {Optizelle Parameters}
        {\lstinputlisting[style=json,linerange=params0-params1]{@ROSENBROCKPATH@/tr_newton.json}}

    \exampleitem
        {Optizelle Output}
        {\lstinputlisting[style=OptizelleOutput]{@OUTPUTPATH@/rosen_output.txt}}
\end{boldlist}

\section{History}

        Optizelle originated in 2010 as a code called PEOpt (Parameter Estimation Using Optimization) written by Joseph Young at Sandia National Laboratories.  There, it was used as the comptuational driver for a variety of both internal and external customers.  Due to the scale of the problems involved and the nuances of high-performance computing environments, PEOpt was designed specifically to integrate with large, existing code bases as quickly and unobtrusively as possible.  Later, Sandia approved the open source release of PEOpt on two separate occasions in 2012 and 2013 under the 2-Clause BSD License.  It was from this released code that Joseph continued work on Optizelle through a new company called OptimoJoe.
        
\chapter{Installation}\label{ch:Install}

        In the following chapter, we discuss how to download, build, and incorporate Optizelle into a new project.

\section{Downloading}

        Optizelle can be downloaded from
\begin{center}
    \mbox{\url{http://www.optimojoe.com/optizelle}}
\end{center}
as a zipped archive.  Here, we also provide direct access to our source code repository.

\section{Dependencies}

        Optizelle depends on the following software packages
\begin{center}\begin{tabular}{|l|l|l|p{.5\textwidth}|}\hline
{\bf Dependency} & {\bf Version} & {\bf Optional?} & {\bf Purpose} \\\hline
\hyperref[tbl:compilers]{C++11} & & No & Compilation of the core library\\\hline
\hyperref[tbl:compilers]{FORTRAN 77} & & No & Determines the name mangling scheme for BLAS and LAPACK\\\hline
\href{http://cmake.org}{CMake} & $\geq$ 2.8.9 & No & Build system\\\hline
\hyperref[tbl:make]{Make} & & No & Build system\\\hline
\href{http://jsoncpp.sourceforge.net}{JsonCpp} & $\geq$ 0.6.0-rc2 & No & Parsing of the configuration files and the default archive/restart engine\\\hline
\hyperref[tbl:blaslapack]{BLAS and LAPACK} & & No & Dense kernels for the default vector spaces in the C++ interface\\\hline
\href{http://www.ctan.org}{pdflatex} & & Yes & Documentation generation \\\hline
\href{http://python.org}{Python} & 2.7.x & Yes & Python interface \\\hline
\href{http://www.numpy.org}{NumPy} & $\geq$ 1.8 & Yes & Dense kernels for the default vector spaces in the Python interface\\\hline
\href{https://www.gnu.org/software/octave}{Octave} & & Yes & MATLAB/Octave interface\\\hline
\href{http://www.mathworks.com/products/matlab}{MATLAB} & & Yes & MATLAB/Octave interface\\\hline
\end{tabular}\end{center}

        As far as C++11 and FORTRAN 77 compilers, in theory, any valid compiler should work.  A small list of these compilers is
\phantomsection\label{tbl:compilers}
\begin{center}\begin{tabular}{|l|l|l|l|}\hline
{\bf Compiler} & {\bf Version} & {\bf Language} & {\bf Platform}\\\hline
\href{http://gcc.gnu.org}{GCC} & $\geq 4.7$ & C++11, FORTRAN 77 & Linux, Unix, OS X \\\hline
\href{http://clang.llvm.org}{Clang} & & C++11 & Linux, Unix, OS X \\\hline
\href{http://dragonegg.llvm.org}{DragonEgg} & & C++11, FORTRAN 77 & Linux, Unix, OS X \\\hline
\href{http://www.mingw.org}{MinGW} & & C++11, FORTRAN 77 & Windows \\\hline
\href{http://www.cygwin.com}{Cygwin} (GCC) & & C++11, FORTRAN 77 & Windows \\\hline
\end{tabular}\end{center}
On Windows, Visual Studio does not compile prepackaged with a FORTRAN 77 compiler.  Though, one can be purchased from a commercial vendor.  As a result, we recommend using either MinGW or Cygwin on Windows.  The primary difference between the two is that Cygwin will create a dependency to the Cygwin runtime libraries during compilation, but MinGW will not.  This becomes important when distributing applications built with Optizelle.  On OS X, the default compilers tend to use old versions of LLVM-GCC (4.2), which is not sufficient to compile Optizelle.  Newer versions of GCC can be found through \href{https://www.macports.org}{MacPorts} or \href{http://hpc.sourceforge.net}{HPC Mac OS X}.

       Next, Optizelle requires a Make utility compatible with CMake.  A small slection of such utilities is 
\phantomsection\label{tbl:make}
\begin{center}\begin{tabular}{|l|l|}\hline
{\bf Make} & {\bf Platform}\\\hline
\href{https://www.gnu.org/software/make}{GNU Make} & Linux, Unix, OS X \\\hline
\href{http://www.mingw.org}{MSYS} & Windows\\\hline
\href{http://www.cygwin.com}{Cygwin} (GNU Make) & Windows \\\hline
\href{http://msdn.microsoft.com/en-us/vstudio}{NMake} & Windows \\\hline
\end{tabular}\end{center}

        Finally, Optizelle is compatible with high-performance varieties of BLAS and LAPACK as long as they contain support for the newer LAPACK functions.  A small subset of these libraries are
\phantomsection\label{tbl:blaslapack}
\begin{center}\begin{tabular}{|l|l|}\hline
{\bf BLAS/LAPACK Library} & \bf{Version}\\\hline
\href{http://www.netlib.org/lapack}{FORTRAN 77 Source} & $\geq$ 3.4.2\\\hline
\href{http://math-atlas.sourceforge.net}{ATLAS} & \\\hline
\href{http://software.intel.com/en-us/intel-mkl}{Intel MKL} & \\\hline
\href{http://developer.amd.com/tools-and-sdks/cpu-development/amd-core-math-library-acml}{AMD ACML} & \\\hline
\href{https://www.tacc.utexas.edu/tacc-projects/gotoblas2}{Goto BLAS} & \\\hline
\end{tabular}\end{center}

\section{Building}

        Optizelle uses CMake as its build system.  On Linux, Unix, OS X, or Cygwin, execute the following commands from the base Optizelle directory:
\begin{enumerate}
    \item \textct{mkdir build}
    \item \textct{cd build}
    \item \textct{ccmake ..}
    \item Configure the build.
    \item \textct{make}
\end{enumerate}
On Windows, if not using Cygwin, execute the following commands:
\begin{enumerate}
    \item Using Windows Explorer, create a directory called \textct{build} in the base Optizelle directory.
    \item Run \textct{cmake-gui.exe}
    \item Set the source directory to the base Optizelle directory.
    \item Set the build directory to the \textct{build} folder created above.
    \item Configure the build.
    \item Build the code (\textct{make} with MSYS.)
\end{enumerate}

\section{Configuring}

        Optizelle provides several different options within CMake in order to customize the build.  We describe these flags in the table below.          
\begin{boldlist}
    \cmakeitem
        {\textctlabel{CMAKE_INSTALL_PREFIX}}
        {\textct{PATH}}
        {Varies}
        {None}
        {None}
        {No}
        {Install location of Optizelle.}

    \cmakeitem
        {\textctlabel{ENABLE_DOCUMENTATION}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {None}
        {\textctref{PDFLATEX_COMPILER},
            \textctref{ENABLE_A4_PAPER}}
        {No}
        {Enables the build of the Optizelle manual from the LaTeX source.
        It builds a pdf file of the manual.}

    \cmakeitem
        {\textctlabel{PDFLATEX_COMPILER}}
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_DOCUMENTATION}}
        {None}
        {Yes}
        {Complete path and executable for pdflatex.}

    \cmakeitem
        {\textctlabel{ENABLE_A4_PAPER}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_DOCUMENTATION}}
        {None}
        {No}
        {When \textct{ON}, the manual uses A4 paper.  Otherwise, the manual uses
        Letter paper.}

    \cmakeitem
        {\textctlabel{ENABLE_CPP}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {None}
        {\textctref{CMAKE_CXX_FLAGS}, 
            \textctref{CMAKE_BUILD_TYPE},
            \textctref{ENABLE_OPENMP}, 
            \textctref{ENABLE_BUILD_BLAS_AND_LAPACK},
            \textctref{ENABLE_BUILD_JSONCPP},
            \textctref{BLAS_LIBRARY},
            \textctref{LAPACK_LIBRARY},
            \textctref{JSONCPP_INCLUDE_DIR},
            \textctref{JSONCPP_LIBRARY},
            \textctref{ENABLE_CPP_EXAMPLES},
            \textctref{ENABLE_CPP_UNIT},
            \textctref{ENABLE_PYTHON},
            \textctref{ENABLE_MATLAB}}
        {No}
        {Enables the Optizelle C++ library.}

    \cmakeitem
        {\textctlabel{CMAKE_CXX_FLAGS}}
        {\textct{STRING}}
        {None}
        {\textctref{ENABLE_CPP}}
        {None}
        {No}
        {C++ compiler specific flags.  Note, we usually require a special flag
        to enable C++11 compilation such as \textct{-std=c++11} on GCC.}

    \cmakeitem
        {\textctlabel{CMAKE_BUILD_TYPE}}
        {\textct{STRING}}
        {None}
        {\textctref{ENABLE_CPP}}
        {None}
        {No}
        {Generally set to either \textct{RELEASE} or \textct{DEBUG}.  Set to
        \textct{RELEASE} for production libraries.  Set to \textct{DEBUG} to
        allow profiling through utilities such as
        \href{http://oprofile.sourceforge.net}{OProfile}.}

    \cmakeitem
        {\textctlabel{ENABLE_OPENMP}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_CPP}}
        {None}
        {No}
        {Enable OpenMP/threaded support for the default, dense vector spaces.
        Note, many BLAS and LAPACK libraries such as those from ATLAS benefit
        from OpenMP directives.}

    \cmakeitem
        {\textctlabel{ENABLE_BUILD_BLAS_AND_LAPACK}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_CPP}}
        {\textctref{LAPACK_ARCHIVE}}
        {No}
        {Builds BLAS and LAPACK from source in case an optimized version is not
        available.}  

    \cmakeitem
        {\textctlabel{LAPACK_ARCHIVE}}
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_BUILD_BLAS_AND_LAPACK}}
        {None}
        {No}
        {Location of the LAPACK archive downloaded from
        \href{http://www.netlib.org/lapack}{Netlib}.}

    \cmakeitem
        {\textctlabel{ENABLE_BUILD_JSONCPP}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_CPP}}
        {\textctref{JSONCPP_ARCHIVE}}
        {No}
        {Builds JsonCpp from source.} 

    \cmakeitem
        {\textctlabel{JSONCPP_ARCHIVE}}
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_BUILD_JSONCPP}}
        {None}
        {No}
        {Location of the JsonCpp archive downloaded from
        \href{http://jsoncpp.sourceforge.net}{SourceForge}.}

    \cmakeitem
        {\textctlabel{BLAS_LIBRARY}}
        {\textct{FILEPATH}}
        {None }
        {\textctref{ENABLE_CPP}}
        {None}
        {Yes }
        {A semicolon separated list of the complete path and library used
        to provide BLAS.  This must include all required libraries in
        order to successfully compile a BLAS dependent application.  For
        example, using ATLAS BLAS, one possible entry is:
        \begin{center}
            \textct{/usr/lib/libf77blas.a;/usr/lib/libatlas.a}
        \end{center}}

    \cmakeitem
        {\textctlabel{LAPACK_LIBRARY} }
        {\textct{FILEPATH}}
        {None }
        {\textctref{ENABLE_CPP}}
        {None}
        {Yes }
        {A semicolon separated list of the complete path and library used
        to provide LAPACK.  This must include all required libraries,
        except for BLAS libraries specified in \textctref{BLAS_LIBRARY}, in
        order to successfully compile a LAPACK dependent application.
        For example, using ATLAS LAPACK, one possible entry is:
        \begin{center}
            \textct{/usr/lib/liblapack.a;/usr/lib/libgfortran.so.3}
        \end{center}}

    \cmakeitem
        {\textctlabel{JSONCPP_INCLUDE_DIR} }
        {\textct{PATH}}
        {None }
        {\textctref{ENABLE_CPP}}
        {None}
        {Yes }
        {A path that indicates where the jsoncpp headers have been 
        installed.  The actual headers must be found in 
        \textct{\$\{JSONCPP_INCLUDE_DIR\}/json/}.}

    \cmakeitem
        {\textctlabel{JSONCPP_LIBRARY}}
        {\textct{FILEPATH}}
        {None }
        {\textctref{ENABLE_CPP}}
        {None}
        {Yes }
        {Complete path and library for JsonCpp. }

    \cmakeitem
        {\textctlabel{ENABLE_CPP_EXAMPLES}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_CPP}}
        {None}
        {No}
        {Enables the build and installation of simple examples that
        demonstrate the use of Optizelle.}

    \cmakeitem
        {\textctlabel{ENABLE_CPP_UNIT}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_CPP}}
        {None}
        {No}
        {Enables the build of unit tests that help validate the 
        Optizelle code.  Execute these unit tests by running
        \textct{ctest} in the CMake build directory.}

    \cmakeitem
        {\textctlabel{ENABLE_PYTHON}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_CPP}}
        {\textctref{PYTHON_INCLUDE_DIR},
            \textctref{PYTHON_LIBRARY},
            \textctref{PYTHON_EXECUTABLE},
            \textctref{ENABLE_PYTHON_EXAMPLES},
            \textctref{ENABLE_PYTHON_UNIT}}
        {No}
        {Enables the build of the Python wrappers for Optizelle.}

    \cmakeitem
        {\textctlabel{PYTHON_INCLUDE_DIR} }
        {\textct{FILEPATH}}
        {None }
        {\textctref{ENABLE_PYTHON}}
        {None}
        {Yes }
        {A path that indicates where the Python 2.7 headers have been 
        installed.  We do not prefix these headers, so we look directly
        in the directory provided here.}

    \cmakeitem
        {\textctlabel{PYTHON_LIBRARY}}
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_PYTHON}}
        {None}
        {Yes}
        {Complete path and library for Python 2.7. }

    \cmakeitem
        {\textctlabel{PYTHON_EXECUTABLE}}
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_PYTHON}}
        {None}
        {Yes}
        {Complete path and executable for Python 2.7. }

    \cmakeitem
        {\textctlabel{ENABLE_PYTHON_EXAMPLES}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_PYTHON}}
        {None}
        {No}
        {Enables the build and installation of simple examples that 
        demonstrate the use of the Python wrappers for Optizelle.}

    \cmakeitem
        {\textctlabel{ENABLE_PYTHON_UNIT}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_PYTHON}}
        {None}
        {No}
        {Enables the build of unit tests that help validate the 
        Python wrappers for the Optizelle code.  Execute these unit
        tests by running \textct{ctest} in the CMake build directory.}

    \cmakeitem
        {\textctlabel{ENABLE_MATLAB}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_CPP}}
        {\textctref{MATLAB_MEX_EXTENSION},
            \textctref{MATLAB_INCLUDE_DIR},
            \textctref{MATLAB_LIBRARY},
            \textctref{MATLAB_EXECUTABLE},
            \textctref{MATLAB_RUN_FLAG},
            \textctref{ENABLE_MATLAB_EXAMPLES},
            \textctref{ENABLE_MATLAB_UNIT}}
        {No}
        {Enables the build of the MATLAB/Octave wrappers for Optizelle.}

    \cmakeitem
        {\textctlabel{MATLAB_MEX_EXTENSION} }
        {\textct{STRING}}
        {None}
        {\textctref{ENABLE_MATLAB}}
        {None}
        {No}
        {Extension of mex files on the system.  This can be found
        by typing in the command 'mexext' inside of MATLAB/Octave.}

    \cmakeitem
        {\textctlabel{MATLAB_INCLUDE_DIR} }
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_MATLAB}}
        {None}
        {Yes }
        {Path that indicates where the MATLAB/Octave header
        \textct{mex.h} has been installed.  We do not prefix these 
        headers, so we look directly in the directory provided here.}

    \cmakeitem
        {\textctlabel{MATLAB_LIBRARY}}
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_MATLAB}}
        {None}
        {Yes}
        {Complete path and library for MATLAB/Octave (\textct{mex}
        for MATLAB and \textct{octinterp} for Octave.)}

    \cmakeitem
        {\textctlabel{MATLAB_EXECUTABLE}}
        {\textct{FILEPATH}}
        {None}
        {\textctref{ENABLE_MATLAB}}
        {None}
        {No}
        {Complete path and executable for MATLAB/Octave. }

    \cmakeitem
        {\textctlabel{MATLAB_RUN_FLAG}}
        {\textct{STRING}}
        {None}
        {\textctref{ENABLE_MATLAB}}
        {None}
        {No}
        {MATLAB/Octave command line flag to run a script (\textct{-r}
        for MATLAB and \textct{--eval} for Octave.) }

    \cmakeitem
        {\textctlabel{ENABLE_MATLAB_EXAMPLES}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_MATLAB}}
        {None}
        {No}
        {Enables the build and installation of simple examples that 
        demonstrate the use of the MATLAB/Octave wrappers for
        Optizelle.}

    \cmakeitem
        {\textctlabel{ENABLE_MATLAB_UNIT}}
        {\textct{BOOL}}
        {\textct{OFF}}
        {\textctref{ENABLE_MATLAB}}
        {None}
        {No}
        {Enables the build of unit tests that help validate the 
        MATLAB/Octave wrappers for the Optizelle code.  Execute these unit
        tests by running \textct{ctest} in the CMake build directory.}
\end{boldlist}

\section{Installing}\label{sec:installing}

    After building Optizelle, installation is as simple as executing
\begin{center}
        \texttt{make install}
\end{center}
from the CMake build directory using GNU Make, MSYS, or Cygwin.  If using a different Make utility, call it on the \textct{install} target.  For a complete list of installed files, see 
\begin{center}
    \textct{install_manifest.txt}
\end{center}
located in the CMake build directory.

    The CMake variable \textctref{CMAKE_INSTALL_PREFIX} determines Optizelle's base installation directory.  From there, we install Optizelle into the following directory tree.  For brevity, we do not list all of the installed files in this diagram, but a selection of the most important files.
\begin{center}\begin{minipage}[t]{1\columnwidth}
    \dirtree{%
        .1 \$\{\textctref{CMAKE_INSTALL_PREFIX}\}.
        .2 lib.
        .3 liboptizelle.a.
        .3 optizelle.lib.
        .3 liboptizelle.so.
        .3 optizelle.dll.
        .2 include.
        .3 optizelle.
        .4 optizelle.h.
        .4 json.h.
        .4 vspaces.h.
        .2 share.
        .3 optizelle.
        .4 doc.
        .5 Optizelle-@CPACK_PACKAGE_VERSION_MAJOR@.@CPACK_PACKAGE_VERSION_MINOR@.@CPACK_PACKAGE_VERSION_PATCH@.pdf.
        .4 python.
        .5 Optizelle.
        .4 matlab.
        .5 setupOptizelle.m.
        .4 examples.
    }
\end{minipage}\end{center}
If the \textctref{ENABLE_BUILD_JSONCPP} variable is set to \textct{ON}, then we also install JsonCpp to the following location.
\begin{center}\begin{minipage}[t]{1\columnwidth}
    \dirtree{%
        .1 \$\{\textctref{CMAKE_INSTALL_PREFIX}\}.
        .2 lib.
        .3 libjson.a.
        .3 libjson.so.
        .2 include.
        .3 json.
        .4 json.h.
    }
\end{minipage}\end{center}
Finally, if the \textctref{ENABLE_BUILD_BLAS_AND_LAPACK} variable is set to \textct{ON}, then we install BLAS and LAPACK to the following location.
\begin{center}\begin{minipage}[t]{1\columnwidth}
    \dirtree{%
        .1 \$\{\textctref{CMAKE_INSTALL_PREFIX}\}.
        .2 lib.
        .3 libblas.so.
        .3 liblapack.so.
    }
\end{minipage}\end{center}

\section{Uninstalling}

        In order to uninstall Optizelle under Linux, Unix, OS X, MSYS, or Cygwin, execute the following command
\begin{center}
    \textct{xargs rm < install_manifest.txt}
\end{center}
from the CMake build directory. 

\section{Distributing}

        Optizelle uses CPack, part of the CMake package, in order to distribute the Optizelle source and compiled binaries.  In order to make a new distributable source package on Linux, Unix, OS X, MSYS, or Cygwin run 
\begin{center}
    \textct{make package_source}
\end{center}
from the CMake build directory.  If using an altnernative Make utility, run it on the target \textct{package_source} from the same directory.  Note, all files in the Optizelle directory tree except for the current build directory are included in the source package with this method.  Specifically, if the directory structure contains multiple build directories, CMake incorporates these in the source package as well, which is probably undesirable. 

        In order to package compiled binaries to other systems, run
\begin{center}
    \textct{cpack}
\end{center}
in the build directory.  This encapsulates Optizelle as well as JsonCpp, BLAS, and LAPACK if they were also built during compilation.

\chapter{Basic API}\label{ch:Basic}

        We organize Optizelle's algorithms into four different categories: 
\begin{center}
    \usebox{\boxOptimizationTypes}
\end{center}
\noindent Since these formulations necessitate different algorithms, we segregate the overall structure of Optizelle and the algorithms themselves into these categories.  
        In order to optimize these formulations, we follow the general procedure:
\begin{enumerate}
    \item \hyperref[sec:import]{\secimport},
    \item \hyperref[sec:importvs]{\secimportvs},
    \item \hyperref[sec:objective]{\secobjective},
    \item \hyperref[sec:constraints]{\secconstraints},
    \item \hyperref[sec:preconditioners]{\secpreconditioners},
    \item \hyperref[sec:state]{\secstate},
    \item \hyperref[sec:params]{\secparams},
    \item \hyperref[sec:accumulatefns]{\secaccumulatefns},
    \item \hyperref[sec:solve]{\secsolve},
    \item \hyperref[sec:extract]{\secextract},
    \item \hyperref[sec:compilerun]{\seccompilerun}.
\end{enumerate}
We discuss how to implement each of the above steps below. 

\section{\secimport}\label{sec:import}

        Each interface uses its own method to import Optizelle:
\begin{filecontents*}{import.cpp}
#include "optizelle/optizelle.h"
#include "optizelle/vspaces.h"
#include "optizelle/json.h"
\end{filecontents*}

\begin{filecontents*}{import.py}
import Optizelle 

import Optizelle.Unconstrained.State
import Optizelle.Unconstrained.Functions
import Optizelle.Unconstrained.Algorithms
import Optizelle.Unconstrained.Restart
import Optizelle.json.Unconstrained

import Optizelle.EqualityConstrained.State
import Optizelle.EqualityConstrained.Functions
import Optizelle.EqualityConstrained.Algorithms
import Optizelle.EqualityConstrained.Restart
import Optizelle.json.EqualityConstrained

import Optizelle.InequalityConstrained.State
import Optizelle.InequalityConstrained.Functions
import Optizelle.InequalityConstrained.Algorithms
import Optizelle.InequalityConstrained.Restart
import Optizelle.json.InequalityConstrained

import Optizelle.Constrained.State
import Optizelle.Constrained.Functions
import Optizelle.Constrained.Algorithms
import Optizelle.Constrained.Restart
import Optizelle.json.Constrained
\end{filecontents*}

\begin{filecontents*}{import.m}
global Optizelle;
setupOptizelle();
\end{filecontents*}

\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++]{import.cpp}}
    \exampleitem
        {Python}
        {\lstinputlisting[style=Python]{import.py}}
    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab]{import.m}}
\end{boldlist}

\noindent In C++, we always require \textct{optizelle/optizelle.h}, but only require \textct{optizelle/json.h} when working with JSON and \textct{optizelle/vspaces.h} when using our default vector spaces such as \textct{Optizelle::Rm} and \textct{Optizelle::SQL}.  In Python, the module required by the structures below can be determined by the element in \textbf{Structure}.  For example, the structure \textct{Optizelle.Unconstrained.State.t} requires the module \textct{Optizelle.Unconstrained.State}.  Finally, in MATLAB/Octave, we encapsulate all of the required functions in the global variable \textct{Optizelle}.

\section{\secimportvs}\label{sec:importvs}
        In the optimization problems
\begin{center}
    \usebox{\boxOptimizationTypes}
\end{center}
\noindent we require that
\begin{align*}
    f :& X\rightarrow\re\\
    g :& X\rightarrow Y\\
    h :& X\rightarrow Z.
\end{align*}
Here, the spaces $X$, $Y$, and $Z$ denote {\it vector spaces}, more specifically, Hilbert spaces.  For most problems, these vector spaces just denote $\re^m$, but we also allow these vector spaces to be spaces of functions such as $L^2(\Omega)$ or matrices such as $\re^{m\times n}$ as long as they contain the necessary operations that we describe in the section \hyperref[sec:customvector]{\seccustomvector}.

        A vector space consists of two separate pieces: the actual vector and the operations required to compute on this vector.  In Optizelle, we maintain this separation.  For $\re^m$, we provide reasonable implementations that use the following:
\begin{boldlist}
    \vsitem
        {C++}
        {\textct{std::vector}}
        {\textct{Optizelle::Rm}}
        {Use as is}
    \vsitem
        {C++}
        {\textct{numpy.array}}
        {\textct{Optizelle.Rm}}
        {Use as is}
    \vsitem
        {MATLAB/Octave}
        {\textct{[]} (column vector)}
        {\textct{Optizelle.Rm}}
        {Use as is}
\end{boldlist}
\noindent To be precise, each of these vector spaces uses the inner product $\langle x,y\rangle=x^Ty$ and defines inequalities pointwise, $x\succeq y \Longleftrightarrow x_i \geq y_i$ for all $1\leq i\leq m$.  Note, we don't require users to use these vector operations in their code.  Simply, if we're happy with using the above vectors, we can use these operations exclusively in Optizelle and forget their details. 

\section{\secobjective}\label{sec:objective}

        In the optimization problems
\begin{center}
    \usebox{\boxOptimizationTypes}
\end{center}
\noindent the function $f:X\rightarrow\re$ denotes the \textit{objective function}.  Note, we restrict ourselves to scalar-valued functions and do not consider multi-objective optimization problems.

        In order to optimize with this function, we require information about its evaluation and derivatives.  Specifically, we require its evaluation, $f(x)$, and gradient, $\nabla f(x)$.  In order to use second-order algorithms, we also require the Hessian-vector product, $\nabla^2 f(x)\delta x$.  In the case that $f:\re^m\rightarrow\re$, we can obtain each of these quantities from its partial derivatives.  Specifically, we write
$$
        f(x)=f(x_1,\dots,x_m).
$$
Then, we have that
\begin{align*}
        \nabla f(x) =& \begin{bmatrix}
            \frac{\partial f}{\partial x_1}(x)\\
            \vdots\\
            \frac{\partial f}{\partial x_m}(x)
        \end{bmatrix},\\
        \nabla^2 f(x)\delta x =& \begin{bmatrix}
            \frac{\partial f}{\partial x_{11}}(x) & \dots & \frac{\partial f}{\partial x_{1m}}(x)\\
            \vdots & \ddots & \vdots\\
            \frac{\partial f}{\partial x_{m1}}(x) & \dots & \frac{\partial f}{\partial x_{mm}}(x)
        \end{bmatrix}\delta x.
\end{align*}
        
        In code, we specify this function as
\begin{boldlist}
    \apiitem
        {C++}
        {\textct{Optizelle::ScalarValuedFunction}}
        {Inheritance}
        {\lstinputlisting[style=C++,linerange={Optizelle0-Optizelle1,ScalarValuedFunction0-ScalarValuedFunction1,Optizelle2-Optizelle3}]{@OPTIZELLECPPPATH@/optizelle.h}}

    \apiitem
        {Python}
        {\textct{Optizelle.ScalarValuedFunction}}
        {Inheritance}
        {\lstinputlisting[style=Python,linerange=ScalarValuedFunction0-ScalarValuedFunction1]{@OPTIZELLEPYTHONPATH@/__init__.py}}

    \apiitem
        {MATLAB/Octave}
        {\textct{Optizelle.ScalarValuedFunction}}
        {Members present}
        {\lstinputlisting[style=Matlab,linerange=ScalarValuedFunction0-ScalarValuedFunction1]{@OPTIZELLEMATLABPATH@/setupOptizelle.m}}
\end{boldlist}
\noindent Note, we require that the Hessian-vector product always be present.  If one is not available, we simply return zero.
        
        As an example, the Rosenbrock function is the function $f:\re^2\rightarrow \re$ where 
$$
        f(x)=(1-x_1)^2+100(x_2-x_1^2)^2.
$$
This function has a gradient of
$$
        \nabla f(x)=\begin{bmatrix}
            -400x_1(x_2-x_1^2)-2(1-x_1)\\
            200(x_2-x_1^2)
        \end{bmatrix}
$$
and Hessian-vector product of 
$$
        \nabla^2 f(x)\delta x=
        \begin{bmatrix}
            1200x_1^2-400x_2+2 & -400x_1\\
            -400x_1 & 200
        \end{bmatrix}\delta x.
$$
Using Optizelle's internal vector spaces, we implement these functions as 
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Objective0-Objective1]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Objective0-Objective1]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Objective0-Objective1]{@ROSENBROCKPATH@/rosenbrock.m}}
\end{boldlist}

        In our simple equality example, we have an objective $f:\re^2\rightarrow \re$ where 
$$
        f(x)=x_1^2+x_2^2
$$
This function has a gradient of
$$
        \nabla f(x)=\begin{bmatrix}
            2x_1\\ 
            2x_2
        \end{bmatrix}
$$
and Hessian-vector product of 
$$
        \nabla^2 f(x)\delta x=
        \begin{bmatrix}
            2 & 0\\
            0 & 2
        \end{bmatrix}\delta x.
$$
We implement this function with the code:
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Objective0-Objective1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Objective0-Objective1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Objective0-Objective1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}

\section{\secconstraints}\label{sec:constraints}
        In the optimization problems
\begin{center}
    \usebox{\boxOptimizationTypes}
\end{center}
\noindent the vector-valued functions $g:X\rightarrow Y$ and $h:X\rightarrow Z$ denote the \textit{equality} and \textit{inequality constraints}, respectfully.

        Here, we allow the equality constraints to be nonlinear, but require that the inequality constraints be affine.  Recall, an affine function is one where $h(\alpha x+(1-\alpha)x)=\alpha h(x)+(1-\alpha)h(y)$ for all $\alpha\in\re$ or equivalently where $h^{\prime\prime}(x)=0$.  Note, we require affine inequality constraints in order to simplify the line search that maintains the nonnegativity of the inequality constraints.  In case we have a nonlinear inequality constraint, we must reformulate the problem in order to make it affine.  The easiest method for doing so is through the reformulations 
$$\left.\begin{array}{rcl}
        \min\limits_{x\in X} && f(x)\\
        \st && h(x)\succeq 0
\end{array}\right\}
\leadsto
\left\{\begin{array}{rcl}
        \min\limits_{x\in X,z\in Z} && f(x)\\
        \st && h(x)-z=0\\
            && z\succeq 0
\end{array}\right.$$
and
$$\left.\begin{array}{rcl}
        \min\limits_{x\in X} && f(x)\\
        \st && g(x)=0\\
            && h(x)\succeq 0
\end{array}\right\}
\leadsto
\left\{\begin{array}{rcl}
        \min\limits_{x\in X,z\in Z} && f(x)\\
        \st && \begin{bmatrix}g(x)\\h(x)-z\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}\\
            && z\succeq 0.
\end{array}\right.$$

        Similar to the objective fucntion, we require derivative information in order optimize effectively.  Specifically, we require the evaluation, $g(x)$, Fr\'{e}chet (total) derivative applied to a vector, $g^\prime(x)\delta x$, and the adjoint of the Fr\'{e}chet derivative applied to a vector, $g^\prime(x)^*\delta y$.  In order to use second order algorithms, we also require the second derivative operation $(g^{\prime\prime}(x)\delta x)^*\delta y$.  Note, we require the same operations from $h$, but since $h$ is affine, $(h^{\prime\prime}(x)\delta x)^*\delta z=0$.  In the case that $g:\re^m\rightarrow\re^n$, the derivation of these derivatives is quite simple.  Here, we write $g$ as
$$
    g(x)=\begin{bmatrix}
        g_1(x)\\
        \vdots\\
        g_n(x)
        \end{bmatrix}.
$$
This means that we have
\begin{align*}
    g^\prime(x)\delta x =& \begin{bmatrix}
        \nabla g_1(x)^T\\
        \vdots\\
        \nabla g_n(x)^T
    \end{bmatrix}\delta x\\
    g^\prime(x)^*\delta y =& \begin{bmatrix}
        \nabla g_1(x) & \dots & \nabla g_n(x)
    \end{bmatrix} \delta y\\
    (g^{\prime\prime}(x)\delta x)^*\delta y =& \sum_{i=1}^n \delta y_i \nabla^2 g_i(x) \delta x.
\end{align*}

        In code, this becomes
\begin{boldlist}
    \apiitem
        {C++}
        {\textct{Optizelle::VectorValuedFunction}}
        {Inheritance}
        {\lstinputlisting[style=C++,linerange={Optizelle0-Optizelle1,VectorValuedFunction0-VectorValuedFunction1,Optizelle2-Optizelle3}]{@OPTIZELLECPPPATH@/optizelle.h}}

    \apiitem
        {Python}
        {\textct{Optizelle.VectorValuedFunction}}
        {Inheritance}
        {\lstinputlisting[style=Python,linerange=VectorValuedFunction0-VectorValuedFunction1]{@OPTIZELLEPYTHONPATH@/__init__.py}}

    \apiitem
        {MATLAB/Octave}
        {\textct{Optizelle.VectorValuedFunction}}
        {Members present}
        {\lstinputlisting[style=Matlab,linerange=VectorValuedFunction0-VectorValuedFunction1]{@OPTIZELLEMATLABPATH@/setupOptizelle.m}}
\end{boldlist}
\noindent Note, we require that the second derivative always be present.  If one is not available, we simply return zero.


        For example, we define a simple equality constraint as 
$$
    g(x)=\begin{bmatrix}
        (x_1-2)^2 + (x_2-2)^2 - 1
    \end{bmatrix}.
$$
Then, we have that
\begin{align*}
    g^\prime(x)\delta x=&\begin{bmatrix}
        2(x_1-2) & 2(x_2-2)
    \end{bmatrix}\delta x\\
    g^\prime(x)^*\delta y=&
        \begin{bmatrix}
            2(x_1-2)\\
            2(x_2-2)
        \end{bmatrix} \delta y\\
    (g^{\prime\prime}(x)\delta x)^*\delta y=&
        \delta y \begin{bmatrix}
            2 & 0\\
            0 & 2
        \end{bmatrix}\delta x
\end{align*}
Using Optizelle's internal vector spaces, we implement these functions as 
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=EqualityConstraint0-EqualityConstraint1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=EqualityConstraint0-EqualityConstraint1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=EqualityConstraint0-EqualityConstraint1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}

\section{\secpreconditioners}\label{sec:preconditioners}

        Since Optizelle is fully matrix-free, its performance depends highly on the quality of the preconditioners provided to it by the user.  To that end, there are two places where preconditioning matters:  the Hessian of the objective function and a KKT system that relates to the equality constraints.  Specifically, we benefit when we can define $P_H:X\rightarrow X$ such that
$$
        P_H \approx \nabla^2 f(x)^{-1}
$$
and $P_l:Y\rightarrow Y$ along with $P_r:Y\rightarrow Y$ such that
$$
        P_l(g^\prime(x)g^\prime(x)^*)P_r\approx I.
$$
        
        Before we discuss these operators in detail, let us emphasize two points.  First, as we describe below, we require only the action of this preconditoner on a vector.  This enables Optizelle to continue to be matrix-free.  Second, even though we use matrix-free abstractions, most of the time, we're better off just using matrices.  At this point in time, both dense and sparse linear algebra libraries are extremely fast.  Unless we have a large PDE constrained optimization problem, just form a matrix of the operator, factor it, and move on.
        
        In the objective function, we use a preconditioner for the Hessian in several different places.  Foremost, we use it to precondition linear systems related to second-order algorithms such as Newton's method.  In addition, we use it within first-order algorithms such as nonlinear-CG.  Certainly, $\nabla^2 f(x)^{-1}$ represents the best such preconditioner, but the Hessian may become singular during the course of optimization, so we must take care in how we generate this preconditioner.  As such, even though $\nabla^2 f(x)$ is self-adjoint, the LU factorization provides a simple, effective manner to factorize the Hessian.  In other words, we find operators $L$ and $U$ such that
$$
        LU=\nabla^2 f(x)
$$
Then, our preconditioner $P_H:X\rightarrow X$ approximates 
$$
        P_H\delta x \approx U^{-1}L^{-1} \delta x.
$$
We say approximate because either $U^{-1}$ or $L^{-1}$ may not exist.  In this case, we note that the action of $U^{-1}$ and $L^{-1}$ on a vector denotes a back and forward solve, respectively.  When the inverse does not exist, we can simply modify these solves to ignore any variables that cause problems.

        For the equality constraints, our algorithms require the repeated solution of a system whose operator is
$$\begin{bmatrix}
        I & g^\prime(x)^*\\
        g^\prime(x) & 0
\end{bmatrix}.$$
As it happens, if $g\prime(x)$ is full-rank, the preconditioner
$$\begin{bmatrix}
        I & 0\\ 
        0 & (g^\prime(x)g^\prime(x)^*)^{-1} 
\end{bmatrix}$$
allows a Krylov method to solve the above system in three iterations.  As such, Optizelle focuses on preconditioning the operator 
$$
        g^\prime(x)g^\prime(x)^*.
$$
Note, unlike the Hessian, we allow both left and right preconditioners for this operator.  In addition, this operator depends on the inner product used by the vector space because it involves an adjoint.  If we're working in $\re^m$ with the inner product $\langle x,y\rangle = x^Ty$, we can ignore this nuance.  Otherwise, we must modify our factorizations to correctly account for the change in inner product.  Outside of this difficulty, we note the operator is always symmetric and positive-semidefinite.  However, like the Hessian, it can and likely will become singular during the course of optimization.  As such, we propose two ways of dealing with this.  In one case, we use a $QR$ factorization of $g^\prime(x)^*$,
$$
        Q R = g^\prime(x)^*,
$$
then form the preconditioners $P_l: Y\rightarrow Y$ and $P_r: Y\rightarrow Y$ where
\begin{align*}
    P_l \delta x \approx& R^{-1}R^{-*}\delta x,\\
    P_r \delta x =& \delta x .
\end{align*}
Again, we must take care in case $R$ is singular.  Alternatively, we can just form $g^\prime(x)g^\prime(x)^*$ and find its $LU$ factorization like we do with the Hessian,
$$
        LU = g^\prime(x)g^\prime(x)^*.
$$
This gives us the preconditioners
\begin{align*}
        P_l\delta x \approx & U^{-1}L^{-1} \delta x,\\
        P_r\delta x = & \delta x.
\end{align*}
In theory, we can use a Choleski factorization to solve this system.  The problem with this approach is that the Choleski factorization will fail when $g^\prime(x)$ is not full rank.  Generally, we find it easier to fix a failing forward or back solve, as is the case with a QR or LU factorization, than to fix a failing factorization.

        In code, we represent preconditioners as a generic linear operator
\begin{boldlist}
    \apiitem
        {C++}
        {\textct{Optizelle::Operator}}
        {Inheritance}
        {\lstinputlisting[style=C++,linerange={Optizelle0-Optizelle1,Operator0-Operator1,Optizelle2-Optizelle3}]{@OPTIZELLECPPPATH@/linalg.h}}

    \apiitem
        {Python}
        {\textct{Optizelle.Operator}}
        {Inheritance}
        {\lstinputlisting[style=Python,linerange=Operator0-Operator1]{@OPTIZELLEPYTHONPATH@/__init__.py}}

    \apiitem
        {MATLAB/Octave}
        {\textct{Optizelle.Operator}}
        {Members present}
        {\lstinputlisting[style=Matlab,linerange=Operator0-Operator1]{@OPTIZELLEMATLABPATH@/setupOptizelle.m}}
\end{boldlist}
As we can see, there is a slight difference when we compare C++ to Python and MATLAB/Octave.  In Python and MATLAB/Octave, we provide the preconditioner with the variable \textct{state} that we describe in the section \hyperref[sec:state]{\secstate}.  We omit this variable in C++.  If we need access to the state in C++, we can simply pass in a reference to it during the operator's construction.  In Python and MATLAB/Octave, this is not an option, so we must pass the state directly.  To be clear, access to the variable \textct{state} is important for most preconditioners.  Recall, we must either evaluate an approximation to $\nabla^2 f(x)^{-1} \delta x$ or $(g^\prime(x)g^\prime(x)^*)^{-1}\delta y$.  When Optizelle calls the preconditioner, it provides $\delta x$ and $\delta y$ and expects $P_H\delta x$, $P_l\delta y$, and $P_r\delta y$ as its return.  Optizelle does \textbf{not} call the preconditioner on the variables $x$ and $y$.  If we want access to these variables, we must find them in the state.

        As another important note, Optizelle can not optimize user defined factorizations.  Meaning, during the course of an optimization iteration, we call these preconditioners several different times at the same optimization iterate, $x$.  As such, if we factorize $\nabla^2 f(x)$ or $g^\prime(x)g^\prime(x)^*$, it is critical to our performance that we cache these factorizations.  The easiest way to tell when a new factorization is needed is to monitor the variable \textct{x} inside of \textct{state}.  This variable represents the current optimization iterate and it does not change until we take a new step in the optimiation algorithms.

        Recall, in our Rosenbrock example, we have a Hessian-vector product of
$$
        \nabla^2 f(x)\delta x=
        \begin{bmatrix}
            1200x_1^2-400x_2+2 & -400x_1\\
            -400x_1 & 200
        \end{bmatrix}\delta x.
$$
This allows us to find the inverse using Cramer's rule
$$
        \nabla^2 f(x)^{-1}\delta x=
        \frac{1}{80000x_1^2-80000x_2+400}
        \begin{bmatrix}
            200 & 400x_1\\
            400x_1 & 1200x_1^2-400x_2+2
        \end{bmatrix}\delta x.
$$
Generally, we claim using Cramer's rule is a bad idea when compared to an LU fractorization, but it works fine on this small example.  Using this formulation, we define our preconditioner to the Hessian with the code 
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Preconditioner0-Preconditioner1]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Preconditioner0-Preconditioner1]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Preconditioner0-Preconditioner1]{@ROSENBROCKPATH@/rosenbrock.m}}
\end{boldlist}

        For our simple equality constraint
$$
    g(x)=\begin{bmatrix}
        (x_1-2)^2 + (x_2-2)^2 - 1
    \end{bmatrix}.
$$
We have that
\begin{align*}
    g^\prime(x)\delta x=&\begin{bmatrix}
        2(x_1-2) & 2(x_2-2)
    \end{bmatrix}\delta x\\
    g^\prime(x)^*\delta y=&
        \begin{bmatrix}
            2(x_1-2)\\
            2(x_2-2)
        \end{bmatrix} \delta y.
\end{align*}
This means that
$$
        g^\prime(x)g^\prime(x)^*\delta y=(4(x_1-2)^2+4(x_2-2)^2)\delta y
$$
and we have a perfect preconditioner
$$
        (g^\prime(x)g^\prime(x)^*)^{-1}\delta y=\frac{1}{4(x_1-2)^2+4(x_2-2)^2}\delta y.
$$
We implement this in our simple equality example with the code
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Preconditioner0-Preconditioner1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Preconditioner0-Preconditioner1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Preconditioner0-Preconditioner1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}

\section{\secstate}\label{sec:state}

        In Optizelle, the optimization state contains an entire description of the current state of the optimization algorithm.  This is unique to the particular optimization formulation, but all algorithms in a particular formulations share the same state.  Most algorithms do not require information about all of these pieces, but they are present to make it easier to switch from one algorithm to another.  For example, trust-region and line-search algorithms share several components, but the trust-region radius is unique to trust-region algorithms and the line-search step length is unique to line-search algorithms.  Nevertheless, we may want to switch from one algorithm to another, so they share the same components.

        In order to define an optimization state, we instantiate the state class within the particular class of formulation we require.  The syntax is

\begin{filecontents*}{state.cpp}
Optizelle::Unconstrained <Real,XX>::State::t state(x);
Optizelle::EqualityConstrained <Real,XX,YY>::State::t state(x,y);
Optizelle::InequalityConstrained <Real,XX,ZZ>::State::t state(x,z);
Optizelle::Constrained <Real,XX,ZZ>::State::t state(x,y,z);
\end{filecontents*}

\begin{filecontents*}{state.py}
state = Optizelle.Unconstrained.State.t(X,msg,x)
state = Optizelle.EqualityConstrained.State.t(X,Y,msg,x,y)
state = Optizelle.InequalityConstrained.State.t(X,Y,msg,x,z)
state = Optizelle.Constrained.State.t(X,Y,Z,msg,x,y,z)
\end{filecontents*}

\begin{filecontents*}{state.m}
state = Optizelle.Unconstrained.State.t(X,msg,x);
state = Optizelle.EqualityConstrained.State.t(X,Y,msg,x,y);
state = Optizelle.InequalityConstrained.State.t(X,Y,msg,x,z);
state = Optizelle.Constrained.State.t(X,Y,Z,msg,x,y,z);
\end{filecontents*}

\begin{boldlist}
    \apiitemshort
        {C++}
        {\lstinputlisting[style=C++]{state.cpp}}
        {Use as is}
    \apiitemshort
        {Python}
        {\lstinputlisting[style=Python]{state.py}}
        {Use as is}
    \apiitemshort
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab]{state.m}}
        {Use as is}
\end{boldlist}

\noindent Here, \textct{XX}, \textct{YY}, and \text{ZZ} in C++ and \textct{X}, \textct{Y}, and \text{Z} in Python and MATLAB/Octave corresponds to the vector spaces $X$, $Y$, and $Z$ described in the section \hyperref[sec:importvs]{\secimportvs}.  Likely, they are just \textct{Optizelle::Rm} or \textct{Optizelle.Rm}.  As a note, We use double letters in C++ to remind us that these spaces denote templates and not objects.  Next, the variable \textct{x} denotes an initial guess for the optimizzation problem.  This guess is very important to the performance of the algorithms, so choose wisely.  The variables \textct{y} and \textct{z} represent arbitrary elements in the codomain of $g$ and $h$, respectively.  We do not use the values of these variables, so any properly allocated vector works fine.  Finally, in Python and MATLAB/Octave, we require a messaging object \textct{msg}.  In the simple case, we can simply use \textct{Optizelle.Messaging}.  For more complicated cases, see the section \hyperref[sec:messaging]{\secmessaging}.

        As an example, we create the optimization state in the Rosenbrock example with the following code:
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=State0-State1]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=State0-State1]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=State0-State1]{@ROSENBROCKPATH@/rosenbrock.m}}
\end{boldlist}
In our simple equality example, we have:
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=State0-State1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=State0-State1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=State0-State1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}

\section{\secparams}\label{sec:params}

        For each optimization problem, the parameters required for an efficient optimization solve can vary wildly.  Nevertheless, the parameters that guide this process reside within the state object.  There are two mechanisms for modifying these entries.  First, the state object created in the section \hyperref[sec:state]{\secstate} is simply an object with a variety of elements that can be modified directly.  Alternatively, and preferably, we can use the JSON reader.  The syntax for reading a parameter file in JSON format from file is
\begin{filecontents*}{json.cpp}
Optizelle::json::Unconstrained <Real,XX>
    ::read(Optizelle::Messaging(),fname,state);
Optizelle::json::EqualityConstrained <Real,XX,YY>
    ::read(Optizelle::Messaging(),fname,state);
Optizelle::json::InequalityConstrained <Real,XX,ZZ>
    ::read(Optizelle::Messaging(),fname,state);
Optizelle::json::Constrained <Real,XX,YY,ZZ>
    ::read(Optizelle::Messaging(),fname,state)
\end{filecontents*}

\begin{filecontents*}{json.py}
Optizelle.json.Unconstrained.read(X,msg,fname,state)
Optizelle.json.EqualityConstrained.read(X,Y,msg,fname,state)
Optizelle.json.InequalityConstrained.read(X,Z,msg,fname,state)
Optizelle.json.Constrained.read(X,Y,Z,msg,fname,state)
\end{filecontents*}

\begin{filecontents*}{json.m}
state = Optizelle.json.Unconstrained.read(X,msg,fname,state)
state = Optizelle.json.EqualityConstrained.read(X,Y,msg,fname,state)
state = Optizelle.json.InequalityConstrained.read(X,Z,msg,fname,state)
state = Optizelle.json.Constrained.read(X,Y,Z,msg,fname,state)
\end{filecontents*}

\begin{boldlist}
    \apiitemshort
        {C++}
        {\lstinputlisting[style=C++]{json.cpp}}
        {Use as is}
    \apiitemshort
        {Python}
        {\lstinputlisting[style=Python]{json.py}}
        {Use as is}
    \apiitemshort
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab]{json.m}}
        {Use as is}
\end{boldlist}
Here, most of the parameters required are identical to those required in the section \hyperref[sec:state]{\secstate}.  The lone, new parameter is \textct{fname}, which corresponds to a string of the file name where we read the JSON formatted parameters.  As to what these parameters are, we discuss that in the chapter \hyperref[ch:parameters]{\chparameters}.

        In our Rosenbrock example, we use the following code to read the optimizzation parameters:
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Parameters0-Parameters1]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Parameters0-Parameters1]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Parameters0-Parameters1]{@ROSENBROCKPATH@/rosenbrock.m}}
\end{boldlist}
This becomes the following in our simple equality example: 
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Parameters0-Parameters1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Parameters0-Parameters1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Parameters0-Parameters1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}

\section{\secaccumulatefns}\label{sec:accumulatefns}

        In order to pass the functions used in the optimization to Optizelle, we accumulate each of them into a bundle of functions.  These bundles are simple structures that contain the appropriate function.  The syntax for creating these objects is

\begin{filecontents*}{functions.cpp}
Optizelle::Unconstrained <Real,XX>::Functions::t fns;
Optizelle::EqualityConstrained <Real,XX,YY>::Functions::t fns;
Optizelle::InequalityConstrained <Real,XX,ZZ>::Functions::t fns;
Optizelle::Constrained <Real,XX,ZZ>::Functions::t fns;
\end{filecontents*}

\begin{filecontents*}{functions.py}
fns = Optizelle.Unconstrained.Functions.t()
fns = Optizelle.EqualityConstrained.Functions.t()
fns = Optizelle.InequalityConstrained.Functions.t()
fns = Optizelle.Constrained.Functions.t()
\end{filecontents*}

\begin{filecontents*}{functions.m}
fns = Optizelle.Unconstrained.Functions.t;
fns = Optizelle.EqualityConstrained.Functions.t;
fns = Optizelle.InequalityConstrained.Functions.t;
fns = Optizelle.Constrained.Functions.t;
\end{filecontents*}

\begin{boldlist}
    \apiitemshort
        {C++}
        {\lstinputlisting[style=C++]{functions.cpp}}
        {Use as is}
    \apiitemshort
        {Python}
        {\lstinputlisting[style=Python]{functions.py}}
        {Use as is}
    \apiitemshort
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab]{functions.m}}
        {Use as is}
\end{boldlist}
\noindent As was in the case in the section \hyperref[sec:state]{\secstate}, \textct{XX}, \textct{YY}, and \text{ZZ} in C++ and \textct{X}, \textct{Y}, and \text{Z} in Python and MATLAB/Octave corresponds to the vector spaces $X$, $Y$, and $Z$ described in the section \hyperref[sec:importvs]{\secimportvs}.  Likely, they are just \textct{Optizelle::Rm} or \textct{Optizelle.Rm}.  

        Now, each of structures contains a number of required and optional elements.  We summarize these elements as follows.
\begin{boldlist}
    \functionitem
        {\textct{f}}
        {\textct{ScalarValuedFunction}}
        {Unconstrained, Equality Constrained, Inequality Constrained, Constrained}
        {Yes}
        {Objective function.}

    \functionitem
        {\textct{PH}}
        {\textct{Operator}}
        {Unconstrained, Equality Constrained, Inequality Constrained, Constrained}
        {No}
        {Preconditioner for the Hessian of the objective function, $\nabla^2 f(x)$.}

    \functionitem
        {\textct{g}}
        {\textct{VectorValuedFunction}}
        {Equality Constrained, Constrained}
        {Yes}
        {Equality constraint.}

    \functionitem
        {\textct{PSchur_left}}
        {\textct{Operator}}
        {Equality Constrained, Constrained}
        {No}
        {Left Schur preconditioner for derivative of the equality constraint, $g^\prime(x)g^\prime(x)^*$.}

    \functionitem
        {\textct{PSchur_right}}
        {\textct{Operator}}
        {Equality Constrained, Constrained}
        {No}
        {Right Schur preconditioner for derivative of the equality constraint, $g^\prime(x)g^\prime(x)^*$.}

    \functionitem
        {\textct{h}}
        {\textct{VectorValuedFunction}}
        {Inequality Constrained, Constrained}
        {Yes}
        {Inequality constraint.}
\end{boldlist}
\noindent In C++, we represent each of the these elements as a \textct{std::unique_ptr} using the type specified above.  In Python, we use simple class elements.  In MATLAB/Octave, we use a structure array.  As a final note, since they are optional, we do \textbf{not} utilize \textct{PH}, \textct{PSchur_left}, or \textct{PSchur_right} by default even when they are defined.  In order to active these functions, we must modify the \textct{PH_type}, \textct{PSchur_left_type}, and \textct{PSchur_right_type} elements in the state, respectively.  We describe these variables in the chapter \hyperref[ch:parameters]{\chparameters}.

        In our Rosenbrock example, we accumlate our functions with the following code
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Functions0-Functions1]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Functions0-Functions1]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Functions0-Functions1]{@ROSENBROCKPATH@/rosenbrock.m}}
\end{boldlist}
\noindent As another example, we accomplish the same task in our simple equality example with the code
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Functions0-Functions1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Functions0-Functions1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Functions0-Functions1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}


\section{\secsolve}\label{sec:solve}

       Once the state, parameters, and functions are set, calling the optimization solver is straightforward.  Simply, we call one of the four commands
\begin{filecontents*}{solver.cpp}
Optizelle::Unconstrained<Real,XX>::Algorithms::getMin(msg,fns,state);
Optizelle::EqualityConstrained<Real,XX,YY>::Algorithms::getMin(msg,fns,state);
Optizelle::InequalityConstrained<Real,XX,ZZ>::Algorithms::getMin(msg,fns,state);
Optizelle::Constrained<Real,XX,YY,ZZ>::Algorithms::getMin(msg,fns,state);
\end{filecontents*}

\begin{filecontents*}{solver.py}
Optizelle.Unconstrained.Algorithms.getMin(X,msg,fns,state)
Optizelle.EqualityConstrained.Algorithms.getMin(X,Y,msg,fns,state)
Optizelle.InequalityConstrained.Algorithms.getMin(X,Z,msg,fns,state)
Optizelle.Constrained.Algorithms.getMin(X,Y,Z,msg,fns,state)
\end{filecontents*}

\begin{filecontents*}{solver.m}
state = Optizelle.Unconstrained.Algorithms.getMin(X,msg,fns,state);
state = Optizelle.EqualityConstrained.Algorithms.getMin(X,Y,msg,fns,state);
state = Optizelle.InequalityConstrained.Algorithms.getMin(X,Z,msg,fns,state);
state = Optizelle.Constrained.Algorithms.getMin(X,Y,Z,msg,fns,state);
\end{filecontents*}

\begin{boldlist}
    \apiitemshort
        {C++}
        {\lstinputlisting[style=C++]{solver.cpp}}
        {Use as is}
    \apiitemshort
        {Python}
        {\lstinputlisting[style=Python]{solver.py}}
        {Use as is}
    \apiitemshort
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab]{solver.m}}
        {Use as is}
\end{boldlist}
\noindent As was in the case in the section \hyperref[sec:state]{\secstate}, \textct{XX}, \textct{YY}, and \text{ZZ} in C++ and \textct{X}, \textct{Y}, and \text{Z} in Python and MATLAB/Octave corresponds to the vector spaces $X$, $Y$, and $Z$ described in the section \hyperref[sec:importvs]{\secimportvs}.  Likely, they are just \textct{Optizelle::Rm} or \textct{Optizelle.Rm}.  Next, we call the function with a messaging object, \textct{msg}.  In the simple case, we can simply use \textct{Optizelle::Messaging()} in C++, \textct{Optizelle.Messaging()} in Python, and \textct{Optizelle.Messaging} in MATLAB/Octave.  For more complicated cases, see the section \hyperref[sec:messaging]{\secmessaging}.  Finally, we pass in the state and bundle of functions that we discussed in the sections \hyperref[sec:state]{\secstate} and \hyperref[sec:accumulatefns]{\secaccumulatefns}, respectively.
        
        In our Rosenbrock example, we call Optizelle's solver with the code:
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Solver0-Solver1]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Solver0-Solver1]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Solver0-Solver1]{@ROSENBROCKPATH@/rosenbrock.m}}
\end{boldlist}
\noindent With the simple equality example, this becomes: 
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Solver0-Solver1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Solver0-Solver1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Solver0-Solver1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}

\section{\secextract}\label{sec:extract}

        After the optimization routine concludes, the solution resides inside of the optimization state in a variable called \textct{x}.  At this point, we can examine our solution and run any post optimization diagnostics we require.

        In our Rosenbrock example, we print out the final solution with the code: 
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Extract0-Extract1]{@ROSENBROCKPATH@/rosenbrock.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Extract0-Extract1]{@ROSENBROCKPATH@/rosenbrock.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Extract0-Extract1]{@ROSENBROCKPATH@/rosenbrock.m}}
\end{boldlist}
\noindent In our simple equality example, this becomes: 
\begin{boldlist}
    \exampleitem
        {C++}
        {\lstinputlisting[style=C++,linerange=Extract0-Extract1]{@SIMPLEEQUALITYPATH@/simple_equality.cpp}}

    \exampleitem
        {Python}
        {\lstinputlisting[style=Python,linerange=Extract0-Extract1]{@SIMPLEEQUALITYPATH@/simple_equality.py}}

    \exampleitem
        {MATLAB/Octave}
        {\lstinputlisting[style=Matlab,linerange=Extract0-Extract1]{@SIMPLEEQUALITYPATH@/simple_equality.m}}
\end{boldlist}
        
\section{\seccompilerun}\label{sec:compilerun}

        As a final step, we need to either compile or run the program.  Each language has its own nuances that we describe below

\subsection{C++}

        As described in the \hyperref[sec:installing]{Installing} section of the manual, we install the C++ relevant headers and libraries to
\begin{center}\begin{minipage}[t]{1\columnwidth}
    \dirtree{%
        .1 \$\{\textctref{CMAKE_INSTALL_PREFIX}\}.
        .2 lib.
        .3 liboptizelle.a.
        .3 optizelle.lib.
        .3 liboptizelle.so.
        .3 optizelle.dll.
        .2 include.
        .3 optizelle.
        .4 optizelle.h.
        .4 json.h.
        .4 vspaces.h.
    }
\end{minipage}\end{center}
Therefore, in order to compile an Optizelle program, we must add the directory
\begin{center}
        \$\{\textctref{CMAKE_INSTALL_PREFIX}\}\textct{/include}
\end{center}
to the list of include directories and
\begin{center}
        \$\{\textctref{CMAKE_INSTALL_PREFIX}\}\textct{/lib}
\end{center}
to the list of library directories.  For the static library, we link either \textct{liboptizelle.a} or \textct{optizelle.lib}.  For the dynamic library, we link either \textct{liboptizelle.so} or \textct{optizelle.dll}.

        Note, Optizelle depends on JsonCpp, BLAS, and LAPACK as well.  Therfore, these headers and libraries must be included in any compilation command as well.  For example, in GCC, we may have the following set of build flags
\begin{center}
        \textct{-I/usr/include -L/usr/lib -loptizelle -ljson -lblas -llapack}
\end{center}
where we assume \textctref{CMAKE_INSTALL_PREFIX}=\textct{/usr}.

\subsection{Python}

        In Python, we install the relevant Python files to the following directory tree:
\begin{center}\begin{minipage}[t]{1\columnwidth}
    \dirtree{%
        .1 \$\{\textctref{CMAKE_INSTALL_PREFIX}\}.
        .2 share.
        .3 optizelle.
        .4 python.
    }
\end{minipage}\end{center}
In order to use Optizelle, we must add this directory to the Python path.  We accomplish this with a command such as
\begin{center}
        \textct{PYTHONPATH=/usr/share/optizelle/python python}
\end{center}
where we assume \textctref{CMAKE_INSTALL_PREFIX}=\textct{/usr}.

\subsection{MATLAB/Octave}

        For MATLAB/Octave, we install Optizelle to the following directory tree:
\begin{center}\begin{minipage}[t]{1\columnwidth}
    \dirtree{%
        .1 \$\{\textctref{CMAKE_INSTALL_PREFIX}\}.
        .2 share.
        .3 optizelle.
        .4 matlab.
    }
\end{minipage}\end{center}
Then, for example, we add this to our path with the command
\begin{center}
        \textct{addpath('/usr/share/optizelle/matlab')}
\end{center}
where we assume \textctref{CMAKE_INSTALL_PREFIX}=\textct{/usr}.  Alternatively, in MATLAB, we can add the path at runtime with the command
\begin{center}
        \textct{MATLABPATH=/usr/share/optizelle/matlab matlab}
\end{center}
In Octave, this becomes
\begin{center}
        \textct{OCTAVE_PATH=/usr/share/optizelle/matlab octave}
\end{center}

\chapter{Optimization Parameters}\label{ch:parameters}

\chapter{Input Specifications}\label{ch:Input}

        The parameters that guide the optimization solver have a dramatic effect on the performance of the internal algorithms.  As such, we catalog these parameters and their affect below. 

\begin{longtable}{llp{0.4\textwidth}}
\multicolumn{3}{p{\textwidth}}{\bf Parameters for: Unconstrained, Inequality Constrained, Equality Constrained, Constrained}\\
Name & Default & Description\\
\texttt{eps\_grad} & $10^{-8}$ 
    & Relative tolerance for the gradient stopping criteria.  In unconstrained problems, Optizelle terminates when $\| \nabla f(x_k) \| \leq \epsilon \| \nabla f(x_0) \|$ where $x_k$ denotes the solution at the $k$-th iteration.  For inequality constrained problems, the gradient stopping criteria is $\| \ineqGradLag{x_k}{z_k} \| \leq \epsilon \| \ineqGradLag{x_0}{z_0}) \|$.  For equality constrained problems, this becomes $\| \eqGradLag{x_k}{y_k} \| \leq \epsilon \| \eqGradLag{x_0}{y_0} \|$.  Finally, for constrained problems, the stopping criteria is $\| \conGradLag{x_k}{y_k}{z_k} \| \leq \epsilon \| \conGradLag{x_0}{y_0}{z_0} \|$.\\
\\
\texttt{eps\_dx} & $10^{-8}$
    & Relative tolerance for the step length stopping criteria.  In unconstrained problems, Optizelle terminates when $\|\delta x\| \leq \epsilon \|\nabla f(x_0)\|$ where $\delta x$ denotes the step in the optimization variables at the current iteration and $x_0$ denotes the initial guess provided to the optimization.  For inequality constrained problems, Optizelle terminates when $\|\delta x\| \leq \epsilon \| \ineqGradLag{x_0}{z_0} \|$.  For equality constrained problems, this becomes $\|\delta x\| \leq \epsilon \| \eqGradLag{x_0}{y_0} \|$.  For constrained problems, this becomes $\|\delta x\| \leq \epsilon \| \conGradLag{x_0}{y_0}{z_0} \|$.\\
\\
\texttt{stored\_history} & 0
    & Number of vectors stored for use with quasi-Newton methods such as SR1 and BFGS\\
\\
\texttt{history\_reset} & 5
    & Number of failed trust-region iterations or line-search batches before the quasi-Newton information is discarded and Optizelle takes a steepest descent direction.\\
\\
\texttt{iter\_max} & 10
    & Maximum number of optimization iterations.\\
\\
\texttt{krylov\_iter\_max} & 10 
    & Maximum number of iterations taken by either truncated-CG or truncated-MINRES when solving the optimality system.\\
\\
\texttt{krylov\_orthog\_max} & 0 
    & Numbers of vectors stored and used in the orthogonalization of truncated-CG or truncated-MINRES.  In exact arithmetic, the number can be provably 0. In practice, if memory is available, it may be worthwhile to over orthogonalize. \\
\\
\texttt{eps\_krylov} & $10^{-2}$
    & Relative stopping criteria for truncated-CG or truncated-MINRES.  In truncated-CG, the stopping criteria when solving the system $Ax=b$ with preconditioner $B$ is $\|B(Ax_k-b)\|\leq \epsilon\|B(Ax_0-b)\|$.  In truncated-MINRES, the stopping criteria is $\|Ax_k-b\|_B \leq \epsilon\|Ax_0-b\|_B$.\\
\\
\texttt{krylov\_solver} & \texttt{ConjugateDirection}
    & Linear system solver used when solving the optimality conditions.  This can either be \texttt{ConjugateDirection} or \texttt{MINRES}.\\
\\
\texttt{algorithm\_class} & \texttt{TrustRegion}
    & Class of algorithm used in optimization.  This can either be \texttt{TrustRegion}, \texttt{LineSearch}, or \texttt{UserDefined}.\\
\\
\texttt{PH\_type} & \texttt{Identity}
    & Preconditioner used when solving the optimality conditions.  This can be either \texttt{Identity}, \texttt{InvBFGS}, or \texttt{InvSR1}, which correspond to the identity operator, a BFGS-based preconditioner, or a SR1 based preconditioner, respectively.\\
\texttt{H\_type} & \texttt{UserDefined}
    & Hessian approximation for the objective function.  This can be either \texttt{UserDefined}, \texttt{Identity}, \texttt{ScaledIdentity}, \texttt{BFGS}, or \texttt{SR1}.  If \texttt{UserDefined} is chosen, Optizelle uses the \texttt{hessvec} calculation inside of the objective function to calculate the Hessian-vector product.  Otherwise, the \texttt{Identity}, \texttt{BFGS}, and \texttt{SR1} approximations corresponds to the identity operator, BFGS, and SR1 operators, respectively.  Finally, the \texttt{ScaledIdentity} option directs Optizelle to use a scaled version of the identity matrix defined as $\|\nabla f(x)\|/(2\Delta) I$ where $\Delta$ denotes the trust-region radius.  This forces the algorithm to take a steepest-descent step the size of the trust-region at every iteration.  This implements a steepest-descent algorithm with a trust-region method. \\
\\
\texttt{msg\_level} & 1
    & Controls the amount of diagnostic output produced by Optizelle.  Here, 0 denotes no output and 1 denotes print information about each optimization iteration.\\
\\
\texttt{delta} & 1.0
    & Initial size of the trust-region radius\\
\\
\texttt{eta1} & 0.1
    & When the actual versus predicted reduction for a trust-region method is below this threshold, the step is rejected.  Otherwise, it is accepted.\\
\\
\texttt{eta2} & 0.9
    & When the actual versus predicted reduction for a trust-region method is above this threshold, the trust-region radius is increased when the trust-region radius inhibits the progress of the algorithm.\\
\\
\texttt{alpha0} & 1.0
    & Initial line-search step length.\\
\\
\texttt{linesearch\_iter\_max} & 5
    & Maximum number of line-search iterations conducted before checking for a reduction in the objective value.\\
\\
\texttt{dir} & \texttt{SteepestDescent}
    & Line-search direction taken by a line-search algorithm.  This can be either \texttt{SteepestDescent}, \texttt{FletcherReeves}, \texttt{PolakRibiere}, \texttt{HestenesStiefel}, \texttt{BFGS}, or \texttt{NewtonCG}.  The \texttt{FletcherReeves}, \texttt{PolakRibiere}, and \texttt{HestenesStiefel} directions corresponds to different flavors of Nonlinear-CG.  The \texttt{BFGS} direction implements a standard BFGS algorithm.  Finally, \texttt{NewtonCG} defines a Newton-CG algorithm that uses the current Hessian approximation defined by \texttt{H\_type} in order to calculate the direction.\\
\\
\texttt{kind} & \texttt{GoldenSection}
    & Kind of line-search algorithm used in a line-search algorithm.  This can be either \texttt{GoldenSection}, \texttt{BackTracking}, \texttt{TwoPointA}, or \texttt{TwoPointB}.  The algorithms \texttt{GoldenSection} and \texttt{BackTracking} correspond to a golden section search and a back tracking line-search, respectively.  \texttt{TwoPointA} and \texttt{TwoPointB} correspond to a line-search based on a two-point approximation of the Hessian.  This is used by the Barzilai-Borwein algorithm for unconstrained optimization.  If either \texttt{TwoPointA} or \texttt{TwoPointB} are chosen, it is important to set \texttt{dir} to \texttt{SteepestDescent}.\\
\\
\multicolumn{3}{p{\textwidth}}{\bf Parameters for: Inequality Constrained, Constrained}\\
Name & Default & Description\\
\texttt{eps\_mu} & $10^{-8}$
    & Absolute tolerance for the interior point stopping criteria.  Optizelle defines the estimate of the infeasibility with respect to the complementary slackness stopping condition as $\langle z,h(x) \rangle/\langle e,e\rangle$ where $z$ denotes the inequality multiplier and $e$ denotes the identity element for the inequality algebra.  For linear bound constraints, this is the vector of all ones, so $\langle e,e\rangle=m$ where $m$ denotes the number of variables.  Hence, the stopping criteria for the inequality constraints is $\mu < \epsilon$.  Since we always choose a starting inequality multiplier so that $\langle z,h(x) \rangle/\langle e,e\rangle=1$, this tolerance is effectively a relative tolerance as well.\\
\\
\texttt{sigma} & $0.5$
    & Rate at which Optizelle attempts to decrease the interior point parameter.  Specifically, if the current estimate of the interior point parameter is $\mu$, then we attempt to reduce $\mu$ to $\sigma \mu$.\\
\\
\texttt{gamma} & $0.95$
    & How close Optizelle allows a step in the optimization algorithm to approach the boundary.  A step of $1.0$ would allow a step to touch the boundary of the inequality constraint in a single step, which is disallowed by the interior point algorithm.\\
\\
\texttt{ipm} & \texttt{PrimalDual}
    & Kind of interior-point method used by Optizelle.  Optizelle allows either \texttt{PrimalDual}, \texttt{PrimalDualLinked}, or \texttt{LogBarrier}.  This corresponds to either a primal-dual interior point method, a primal-dual interior point method where the primal and dual step lengths are linked together, or a log-barrier algorithm.\\
\\
\texttt{cstrat} & \texttt{Constant}
    & Centrality strategy used by the interior-point method.  Optizelle allows either \texttt{Constant}, \texttt{StairStep}, or \texttt{PredictorCorrector} strategies.  This corresponds to a constant reduction in the interior point parameter by $\sigma$, reducing $\sigma$ only when the relative reduction in the interior point parameter $\mu$ is less than the relative reduction in the gradient stopping condition, or a predictor-corrector strategy where on even iterations $\sigma=0$ and on odd iterations $\sigma=1$.\\
\\
\multicolumn{3}{p{\textwidth}}{\bf Parameters for: Equality Constrained, Constrained}\\
Name & Default & Description\\
\texttt{zeta} & 0.8 & Fraction of the trust-region used for the quasi-normal step.\\
\\
\texttt{eta0} & 0.5 & Trust-region parameter that bounds the error in the predicted-reduction.\\
\\
\texttt{rho} & 1.0 & Initial penalty parameter used in the augmented Lagrangian.  For globalization purposes, the equality constrained algorithm uses an augmented Lagrangian of the form $f(x)+\langle y,g(x)\rangle+\rho\|g(x)\|^2$.  For the constrained algorithms, Optizelle uses an augmented Lagrangian of the form $f(x)+\langle y,g(x)\rangle+\rho\|g(x)\|^2-\mu \textnormal{barr}(h(x))$ where $\textnormal{barr}(x)$ denotes the barrier functioned defined in the inequality algebra.\\
\\
\texttt{rho\_bar} & $10^{-8}$ & Fixed increase in the penalty parameter in the augmented Lagrangian merit function.\\
\\
\texttt{eps\_constr} & $10^{-8}$ & Relative stopping tolerance for feasibility with respect to the equality constraint.  Optizelle requires that $\|g(x_k)\|<\epsilon\|g(x_0)||$ in order to terminate where $x_k$ denotes the solution at the $k$-th iteration.\\
\\
\texttt{xi\_all} & $10^{-4}$ & Relative stopping tolerance for all of the augmented system solves, \texttt{xi\_qn}, \texttt{xi\_pg}, \texttt{xi\_proj}, \texttt{xi\_proj}, \texttt{xi\_tang}, and \texttt{xi\_lmh}, described below.\\
\\
\texttt{xi\_qn} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the quasi-Newton step.\\
\\
\texttt{xi\_pg} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the projection of the gradient prior to solving the tangential subproblem.\\
\\
\texttt{xi\_proj} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the null-space projection of the iterate in the tangential subproblem.\\ 
\\
\texttt{xi\_tang} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the tangential step computation after solving the tangential subproblem. \\
\\
\texttt{xi\_lmh} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the equality multiplier computation.\\
\\
\texttt{xi\_lmg} & $10^4$ & Absolute tolerance on the residual of the equality multiplier solve. \\
\\
\texttt{xi\_4} & 2.0 & Tolerance for how much error is acceptable after computing the tangential step given the result from the tangential subproblem.\\
\\
\texttt{augsys\_iter\_max} & 100 & Maximum number of GMRES iterations allowed when solving an augmented system.\\
\\
\texttt{augsys\_rst\_freq} & 0 & Restarts GMRES every specified number of iterations in order to save memory.  When 0, Optizelle uses no restarting.\\
\\
\texttt{PSchur\_left\_type} & \texttt{Identity} & Type of left Schur preconditioner used by Optizelle during an augmented system solve.  This can be either \texttt{Identity} or \texttt{UserDefined}.  An augmented system solve finds the solution of a linear system where the operator has the form $\begin{bmatrix} I & g^\prime(x)^*\\g^\prime(x) & 0\end{bmatrix}$.  The Schur preconditioner for this problem is an operator of the form $\begin{bmatrix} I & 0\\ 0 & B\end{bmatrix}$ where $B$ is the operator specified by this parameter.  Note, if we specify $B=(g^\prime(x)g^\prime(x)^*)^{-1}$, then the augmented system solves in 3 iterations.\\
\\
\texttt{PSchur\_right\_type} & \texttt{Identity} & Type of right Schur preconditioner used by Optizelle during an augmented system solve.  This can be either \texttt{Identity} or \texttt{UserDefined}.
\end{longtable}

\chapter{Output}\label{ch:Output}

        Optizelle generates a series of diagnostics while running that give information about the behavior and performance of the underlying algorithm.  This information is organized into columns that are exactly 10 characters wide.  Each column always has some sort of information, which makes the output easy to parse using standard Unix utilities such as \texttt{cut} or \texttt{awk}.  As far as the information in the columns themselves, we detail their meaning in the following table.

\begin{longtable}{lp{0.8\textwidth}}
\multicolumn{2}{p{\textwidth}}{\bf Output for: Unconstrained, Inequality Constrained, Equality Constrained, Constrained}\\
Name & Description\\
\texttt{Iter} & Current optimization iteration.  If the value of this entry is \texttt{*}, that means that a trust-region algorithm has rejected a step due to an unfavorable actual versus predicted reduction. \\
\\
\texttt{f(x)} & Value of the objective function at the start of the specified iteration.\\
\\
\texttt{merit(x)} & Value of the merit function at the start of the iteration.  For an unconstrained problem, this is simply $f(x)$.  For an inequality constrained problem, Optizelle returns $f(x)-\mu \textnormal{barr}(x)$ where $\textnormal{barr}(x)$ denotes the value of the barrier function from the inequality algebra.  For an equality constrained problem, Optizelle gives $f(x)-\langle y, g(x)\rangle + \rho \|g(x)\|^2$.  For a constrained problem, Optizelle defines this quantity as $f(x)-\langle y, g(x)\rangle + \rho \|g(x)\|^2 - \mu\textnormal{barr}(x)$.\\
\\
\texttt{||grad||} & Norm of the gradient of either the objective function or the Lagrangian.  In each case, this denotes the norm of the optimality conditions.  In the unconstrained case, Optizelle defines this quantity as $\|\nabla f(x)\|$.  In the inequality constrained case, this becomes $\|\ineqGradLag{x}{z}\|$.  In the equality constrained case, this is $\|\eqGradLag{x}{y}\|$.  In the fully constrained case, this quantity denotes $\|\conGradLag{x}{y}{z}\|$.\\\\
\texttt{||dx||} & Norm of the step taken during the last iteration. \\
\\
\texttt{ared} & Actual reduction defined as $\textnormal{merit}(x_k)-\textnormal{merit}(x_{k+1})$ where $x_k$ denotes the iterate at the previous step and $x_{k+1}$ denotes the current iterate.\\
\\
\texttt{pred} & Predicted reduction defined as $\textnormal{merit}(x_k)-\textnormal{model}(\delta x)$ where $x_k$ denotes the iterate at the previous step and $\delta x$ denotes the current step.  Depending on the formulation, Optizelle uses a different model, but these models generally correspond to a quadratic approximation to the merit function.\\
\\
\texttt{ared/pred} & Actual versus predicted reduction.  For a perfect model, this ratio is 1.0.\\
\\
\texttt{KryIter} & Number of iterations used by truncated-CG or MINRES when solving the optimality system.\\
\\
\texttt{KryErr} & Error in truncated-CG or MINRES when solving the optimality system.  For truncated-CG, this is defined as $\|B(Ax-b)\|$ where we are solving the system $Ax=b$ with the preconditioner $B$.  For truncated-MINRES, this becomes $\|Ax-b\|_B$.\\
\\
\texttt{KryWhy} & Why the truncated-CG or MINRES solve terminated.  This value can be either \texttt{RelErrSml}, the relative error is small, \texttt{NegCurv}, detected negative curvature, \texttt{IterExcd}, number of iterations exceeded, \texttt{TrstReg}, step violates the trust-region, \texttt{Unstable}, loss of orthogonality in the system, or \texttt{InvldCnt}, an invalid center for the trust-region radius.\\
\\
\texttt{LSIter} & Number of iterations taken by the line search.\\
\\
\multicolumn{2}{p{\textwidth}}{\bf Output for: Inequality Constrained, Constrained}\\
Name & Description\\
\texttt{mu} & Interior point parameter used by the optimization.  In other words, the target interior point parameter.\\
\\
\texttt{mu\_est} & Current estimate of the interior point parameter defined as $\langle z,h(x) \rangle/\langle e,e\rangle$ where $z$ denotes the inequality multiplier and $e$ denotes the identity element for the inequality algebra.  For linear bound constraints, this is the vector of all ones, so $\langle e,e\rangle=m$ where $m$ denotes the number of variables.\\
\\
\multicolumn{2}{p{\textwidth}}{\bf Output for: Equality Constrained, Constrained}\\
Name & Description\\
\texttt{||g(x)||} & Norm of the equality constraint at the start of the optimization iteration.  This denotes the infeasibility of the current iterate with respect to the equality constraints. 
\end{longtable}

\chapter{Advanced API}\label{ch:Advanced}

        Optizelle contains many additional features such as customizing the output and defining custom vector spaces.  We detail these features below.

\section{\secmessaging}\label{sec:messaging}

        Optizelle messaging objects inherit from \texttt{Optizelle::Messaging}, which Optizelle defines as
\input{messagingSpecification.tex}
Hence, messaging objects have two functions, \texttt{print} and \texttt{error}, that both accept a string and print the result.  The only difference between \texttt{print} and \texttt{error} is that \texttt{error} must safely exit the program.  Within a larger program, it is common to modify the messaging object so that Optizelle's output goes directly to a specified file.
        
\section{Symmetric Cone Programming}        
        In the case of C++, we also provide a built-in vector space for semidefinite, second-order cone, and linear (SQL) programs:
\begin{boldlist}
    \apiitem
        {C++}
        {\textct{Optizelle::SQL}}
        {Use as is}
        {\lstinputlisting[style=C++,linerange={Optizelle0-Optizelle1,Cone0-Cone1,SQL0,SQL1,Optizelle2-Optizelle3}]{@OPTIZELLECPPPATH@/vspaces.h}}
\end{boldlist}

\noindent Take special note of the constructor:

\begin{flushleft}
    \lstinputlisting[style=C++,linerange={SQLConstructor0-SQLConstructor1}]{@OPTIZELLECPPPATH@/vspaces.h}
\end{flushleft}
\noindent In order to construct an SQL vector, we must provide a vector of \textct{Cone::t}s that describe the cones and a vector of \textct{Natural}s that describe the sizes.  In order to access these elements on the vector \textct{x}, we use the following indexing functions
\begin{center}\begin{tabular}{llll}
Number of cones & Type of cone & Type of Indexing & Use\\\hline
Single  & Quadradic/Linear & Specific element & \textct{x(i)}\\
Multiple  & Quadradic/Linear & Specific element & \textct{x(k,i)}\\
Multiple & Semidefinite & Specific element & \textct{x(k,i,j)}\\
Multiple & Semidefinite/Quadratic/Linear & First element & \textct{x.front(k)}\\
Multiple & Quadratic & First element & \textct{x.naught(k)}\\
Multiple & Quadratic & Second element & \textct{x.bar(k)}
\end{tabular}\end{center}
\noindent Finally, we have a couple of query functions
\begin{center}\begin{tabular}{lll}
Purpose & Use\\\hline
Size of block & \textct{x.blkSize(k)}\\
Type of block & \textct{x.blkType(k)}\\
Number of blocks & \textct{x.numblocks()}
\end{tabular}\end{center}

        Outside of Rm, we also allow these vector spaces of functions such as $L^2(\Omega)$ or martrices such as $\re^{m\times n}$ as long as these spaces provide sufficient vector-space operations.  Specifically, for $W\in\{X,Y,Z\}$, we require that $W$ provide the functions:
\begin{center}\begin{tabular}{|lll|}\hline
    \textbf{Name} & \textbf{Definition} & \textbf{Languages}\\\hline
    \textct{init(x)} & $\textit{init}\leftarrow \xi(W)$ & C++, Python, MATLAB/Octave\\\hline

    \textct{copy(x,y)} & $y\leftarrow x$ & C++, Python\\
    \textct{copy(x)} & $\textit{copy}\leftarrow x$ & MATLAB/Octave\\\hline

    \textct{scal(alpha,x)} & $x\leftarrow \alpha x$ & C++, Python\\
    & $\textit{scal}\leftarrow \alpha x$ & MATLAB/Octave\\\hline

    \textct{axpy(alpha,x,y)} & $y\leftarrow \alpha x+y$ & C++, Python\\
    & $\textit{axpy}\leftarrow \alpha x+y$ & MATLAB/Octave\\\hline

    \textct{innr(x,y)} & $\textit{innr}\leftarrow \langle x,y\rangle$ & C++, Python, MATLAB/Octave\\\hline
    
    \textct{zero(x)} & $x\leftarrow 0$ & C++, Python\\
    & $\textit{zero}\leftarrow 0$ & MATLAB/Octave\\\hline
    
    \textct{rand(x)} & $x\leftarrow \psi(W)$ & C++, Python\\
    & $\textit{rand}\leftarrow \psi(W)$ & MATLAB/Octave\\\hline
\end{tabular}\end{center}
Here, the notation $x\leftarrow \alpha x$ means we assign the element $x$ the value $\alpha x$ whereas the notation $\textit{scal}\leftarrow \alpha x$ means we return the result $\alpha x$.  Next, $\xi,\psi:\mathcal{P}(W)\rightarrow W$ denote choice functions where $\xi$ and $\psi$ are typically deterministic and stochastic, respectfully.  In other words, $\xi$ and $\psi$ choose either fixed or random elements of $W$.  More specifically, we use \textct{init} to allocate memory and \textct{rand} to conduct diagnostic tests.


\section{\seccustomvector}\label{sec:customvector}

        Optizelle allows the use of a custom vector space provided by the user.  This allows Optizelle to more easily work with programs that store their information in a custom format.  Simply, we define a new vector space rather than trying to convert the internal data into a \texttt{std::vector}.  The requirements for a new vector space are a structure of the form
\input{vectorSpaceSpecification.tex}
Note, the functions \texttt{prod}, \texttt{id}, \texttt{linv}, \texttt{barr}, \texttt{srch}, and \texttt{symm} are only required for the codomain of the inequality constraint.  We explain these functions below.

\subsection{typedef}

       Optizelle requires that the custom vector space contains a typedef called Vector.  The underlying storage for the vector can be any valid class as long as the class contains a default constructor that does nothing and a copy constructor that knows how to copy items created by the default constructor.  As far as allocating memory for this structure, this is does with the \texttt{init} command that we describe below.

\subsection{init}

        Since allocating memory for a custom object may be complicated, Optizelle uses an \texttt{init} function that copies the structure of a vector from an already existing object.  Hence, this function can be seen as a deep copy from x to y.

\subsection{copy}
        
        The \texttt{copy} function copies the data from x to y using already initialized vectors.  Hence, it implements a shallow copy.

\subsection{scal}

        The \text{scal} function computes and returns the scalar multiple of a vector.  The scalar argument has the same type as the template argument to the structure.

\subsection{zero}

        The \texttt{zero} function transforms a vector into the zero element for the vector space.  In other words, the additive identity element.  The reason that \texttt{zero} exists separately from \texttt{scal} is that if a vector contains NaN elements, the multiplication of zero by NaN is undefined.

\subsection{axpy}

        Next, the function \texttt{axpy} computes the operator $\alpha x + y$ and then stores the result in $y$.

\subsection{innr}

        The inner product function \texttt{innr}, computes the inner product between two elements and then returns the result.  Recall, the inner product is a function $\langle \cdot,\cdot \rangle\rightarrow \re$ so that
\begin{enumerate}
    \item $\langle x,y\rangle = \langle y,x \rangle$,
    \item $\langle \alpha x,y\rangle = \alpha \langle x,y\rangle$,
    \item $\langle x+y,z\rangle = \langle x,z\rangle + \langle y,z\rangle$,
    \item $\langle x,x\rangle\geq 0$ with equality only when $x=0$.
\end{enumerate}
Note, Optizelle restricts the formulations to real vector spaces.  Note, for $\ell^2$, the inner product can be defined simply as $\langle x,y\rangle=x^Ty$.

\subsection{prod}

        The function \texttt{prod} defines a pseudo-Jordan product between two elements.  We say pseudo-Jordan in the sense that we do not require a full Euclidean-Jordan algebra.  Instead, we drop the requirement for commutativity.  Hence, for linear bound constraints, we define that
$$
        [x\circ y]_i = x_iy_i.
$$
Hence, the product denotes the pointwise or Hadamard product.  For second-order cone constraints, we define that
$$
        \begin{bmatrix}x_0\\\bar{x}\end{bmatrix} \circ \begin{bmatrix}y_0\\\bar{y}\end{bmatrix}=\begin{bmatrix} x_0y_0 + \bar{x}^T\bar{y}\\x_0 \bar{y} + y_0 \bar{x}\end{bmatrix}.
$$
For semidefinite programming, we have that
$$
        X\circ Y = XY.
$$
Alternatively, we can define that
$$
        X\circ Y = \frac{XY + YX}{2},
$$
but the inverse operation we discuss below will be far more inefficient.

\subsection{id} 

       The function \texttt{id} transforms a vector into the identity element for the inequality algebra.  Hence, it returns an element $e$ so that $x\circ e=x$.   For linear bound constraints, $e$ denotes the vector of all ones.  For second-order cone constraints, $e=\begin{bmatrix} 1 & 0 & \dots & 0\end{bmatrix}^T$.  For semidefinite constraints, $e=I$.

\subsection{linv}

        The function \texttt{linv} denotes the inverse operation to \texttt{prod}.  Note, $\texttt{prod}$ defines a bilinear operation so that there exists a linear operator $L(x)$ such that $x\circ y=L(x)y$.  The function \texttt{linv} computes the action of the {\it inverse} of $L(x)$ on a vector.  For linear bound constraints, $L(x)=\textnormal{Diag}(x)$, where $\textnormal{Diag}(x)$ denotes the diagonal matrix with $x$ on the diagonal.  For second-order cone constraints, $L(x)=\textnormal{Arw}(x)$ where we define $\textnormal{Arw}(x)$ as
$$
        \textnormal{Arw}\left(\begin{bmatrix}x_0\\\bar{x}\end{bmatrix}\right) =
\begin{bmatrix}
        x_0 & \bar{x}^T\\\bar{x} & x_0 I
\end{bmatrix}.
$$
For semidefinite constraints, we can either define that $L(X)=X$ or that $L(X)=\frac{X\cdot + \cdot X}{2}$.  Generally, it is preferable to use the first definition since $L(X)^{-1}=X^{-1}$.  In the second case, Optizelle requires the solution of the Sylvester equations.

\subsection{barr}

        The function \texttt{barr} denotes the barrier function used by the merit function.  Optizelle requires a function so that
$$
    x\circ \nabla \textnormal{barr}(x) = e.
$$
For linear bound constraints, this is simply the log-barrier function
$$
    \textnormal{barr}(x) = \sum\limits_{i=1}^m \log(x_i).
$$
For second-order cone constraints, we define this as
$$
    \textnormal{barr}\left(\begin{bmatrix}x_0\\\bar{x}\end{bmatrix}\right) = \frac{1}{2} \log(x_0^2-\langle \bar{x},\bar{x}\rangle).
$$
For semidefinite constraints, we define this as
$$
    \textnormal{barr}(X)=\log(\textnormal{det}(X))
$$
where $\textnormal{det}(X)$ denotes the determinant of $X$.

\subsection{srch}

        The function \texttt{srch} denotes the search function used to maintain strict feasibility with respect to the inequality constraint.  We define this as
$$
    \textnormal{srch}(x,y)=\arg\max\limits_{\alpha\geq 0} \{ \alpha\in\re : \alpha x + y \succeq 0\}
$$
where we assume $y\succ 0$.  Hence, $\alpha$ denotes the maximum distance Optizelle can move in the direction $x$ from $y$ so that $\alpha x + y$ remains feasible.

\subsection{symm}

        The function \texttt{symm} denotes the symmetrization operator.  Optizelle requires this operator since we relax the commutativity requirement from the Euclidean-Jordan algebra.  For linear bound constraints and second-order cone constraints, this operation does nothing.  In addition, for semidefinite constraints where $X\circ Y=\frac{XY+YX}{2}$, this operation does nothing.  However, for semidefinite constraints where $X\circ Y=XY$, Optizelle requires $\textnormal{symm}(X)=\frac{X+X^T}{2}$.

\end{document}
