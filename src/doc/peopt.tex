\documentclass{report}

% All of the packages required
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{url}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{amsmath}

% Change the figure numbering to be chap.sec.num
\renewcommand{\thefigure}{\arabic{chapter}.\arabic{section}.\arabic{figure}}

% Simple macros to help printing
\newcommand{\re}{\mathbb{R}}

% Create some new environments for displaying code and output
\lstdefinelanguage{peoptOutput}{
    morekeywords={Iter,f,x,merit,grad,dx,KryWhy,KryErr,KryIter,ared,pred,
        ared/pred,mu,mu_est,g},
    sensitive=false,
    literate=
}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}
\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {e-}{{{\color{numb}e-}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\lstdefinestyle{C++}{
    language=C++,
    backgroundcolor=\color{black!5},
    showstringspaces=false,
    basicstyle=\footnotesize
} 

\lstdefinestyle{peoptOutput}{
    language=peoptOutput,
    backgroundcolor=\color{black!5},
    basicstyle=\scriptsize
}

\lstdefinestyle{json}{
    language=json,
    backgroundcolor=\color{black!5},
    basicstyle=\footnotesize
}

% Create some Lagrangian macros
\newcommand{\ineqGradLag}[2]{\nabla f(#1)-h^\prime(#1)^*#2}
\newcommand{\eqGradLag}[2]{\nabla f(#1)+g^\prime(#1)^*#2}
\newcommand{\conGradLag}[3]{\nabla f(#1)+g^\prime(#1)^*#2-h^\prime(#1)^*#3}

\title{Parameter Estimation Using Optimization (PEOpt)}
\author{Joseph Young}
\begin{document}
\maketitle
\tableofcontents

\chapter{Introduction}

        The PEOpt (\underline{P}arameter \underline{E}stimation Using \underline{Opt}imization) library contains a collection of routines used to solve a variety of nonlinear optimization problems.  It focuses on open ended abstractions that make it easy to interface to existing applications.

\subparagraph{Algorithms}
        PEOpt provides the following algorithms 
\begin{center}\begin{tabular}{l|p{0.6\textwidth}}
Problem Class & Algorithms\\\hline
Unconstrained & Steepest Descent, Preconditioned Nonlinear-CG (Fletcher-Reeves, Polak-Ribiere, Hestenes-Stiefel), BFGS, Newton-CG, SR1, Trust-Region Newton (Truncated-CG and Truncated-MINRES), Barzilai-Borwein Two-Point Approximation\\\\
Equality Constrained & Inexact Composite Step SQP\\\\
Inequality Constrained & Primal-Dual Interior Point Method for Cone Constraints (Linear, Second-Order Cone, and Semidefinite), Log-Barrier Method for Cone Constraints\\\\
Constrained & Any combination of the above .
\end{tabular}\end{center}
Changing from one algorithm to another is as simple as changing one parameter in the input deck.

\subparagraph{Extensible Linear Algebra}
Although PEOpt provides a default interface where the optimization variables are represented by C++ STL vectors, PEOpt's vector and linear algebra can be fully replaced by the user.  This means PEOpt can use arbitrary vectors, structures, or classes as its optimization variables.  This includes distributed vectors that use MPI.  In addition, PEOpt may utilize user defined preconditioning software provided by the user.  Together, this allows PEOpt to more easily integrate into an existing software package.

\subparagraph{Multiple Programming Languages} PEOpt provides interfaces for C++, MATLAB, and Python.

\subparagraph{Sophisticated Control of the Optimization Algorithm} PEOpt allows the user to insert arbitrary code into the optimization algorithm.  This allows  custom processing code to manipulate the optimization iterates between iterations.  For example, in signal processing applications, the optimization iterates can be run through a band-pass filter at the end of each optimization iteration.

\subparagraph{Customized Restarts} PEOpt supports user defined restart procedures, which allow the entire state of the optimization algorithm to be transferred to the user and then written to disk.  This allows PEOpt to stopped and started at the user convenience.  It also enables sophisticated diagnostics to be run on the performance of the optimization algorithms.

        Simple examples of PEOpt applications are provided below.

\section{Unconstrained Optimization}

        The PEOpt unconstrained optimization routines solve problems of the form
$$
        \min_{x\in X} f(x)
$$
where $f:X\rightarrow \re$ denotes a differentiable function.  For example, we formulate the minimization of the Rosenbrock function as
$$
        \min_{x\in\re^2} (1-x_1)^2+100(x_2-x_1^2)^2.
$$
In PEOpt, we minimize the Rosenbrock function with the code in Listing \ref{lst:Rosen} and the input specification in Listing \ref{lst:RosenJSON}, which generates the output in Listing \ref{lst:RosenOut}.  We explain the functions used to generate this example in Chapter \ref{ch:Basic}, the input specification in Chapter \ref{ch:Input}, and the output in Chapter \ref{ch:Output}.

\input{rosen_code.tex}
\input{rosen_input.tex}
\input{rosen_output.tex}

\section{Inequality Constrained Optimization}

        The PEopt inequality constrained optimization routines solve problems in the form
$$
        \min_{x\in X} \{ f(x) : h(x)\succeq 0\}
$$
where $f:X\rightarrow \re$ denotes a differentiable function, $h:X\rightarrow Z$ denotes an affine function, and $\succeq$ denotes a partial order that corresponds to a symmetric cone.  A symmetric cone includes inequalities such as pointwise inequalities, second-order cones, and semidefinite cones.  For example, these problems include linearly constrained quadratic programs such as
$$
        \min_{x\in\re^2}\{(x_1+1)^2+(x_2+1)^2 : x_1 + 2x_2 \geq 1, 2x_1 + x_2\geq 1\}
$$
In PEOpt, we minimize the above inequality constrained problem with the code in Listing \ref{lst:simpleIneq} and the input specification in Listing \ref{lst:simpleIneqJSON}, which generates the output in Listing \ref{lst:simpleIneqOut}.  As with the unconstrained example, we explain the functions used to generate this example in Chapter \ref{ch:Basic}, the input specification in Chapter \ref{ch:Input}, and the output in Chapter \ref{ch:Output}.

\input{simple_ineq_code.tex}
\input{simple_ineq_input.tex}
\input{simple_ineq_output.tex}

\section{Equality Constrained Optimization}

        The PEOpt equality constrained optimization routines solve problems in the form
$$
        \min_{x\in X} \{ f(x) : g(x)=0 \}
$$
where $f:X\rightarrow \re$ and $g:X\rightarrow Y$ denote differentiable functions.  For example, these problems include linearly constrained quadratic programs such as
$$
        \min_{x\in\re^2}\{x_1^2+x_2^2 : (x_1-2)^2 + (x_2-2)^2 = 1 \}
$$
In order to minimize this formulation in PEOpt, we use the code in Listing \ref{lst:simpleEq} and the input specification in Listing \ref{lst:simpleEqJSON}, which generates the output in Listing \ref{lst:simpleEqOut}.  Similar to the previous two examples, we explain the functions used to generate this example in Chapter \ref{ch:Basic}, the input specification in Chapter \ref{ch:Input}, and the output in Chapter \ref{ch:Output}.

\input{simple_eq_code.tex}
\input{simple_eq_input.tex}
\input{simple_eq_output.tex}

\section{Constrained Optimization}

        The PEOpt constrained optimization routines solve problems in the form
$$
        \min_{x\in X} \{ f(x) : g(x)=0, h(x)\succeq 0 \}
$$
where $f:X\rightarrow \re$ and $g:X\rightarrow Y$ denote differentiable functions, $h:X\rightarrow Z$ denotes an affine function, and $\succeq$ denotes a partial order that corresponds to a symmetric cone.  For example, these formulations include constrained problems such as 
$$
        \min_{x\in\re^2}\{(x_1+1)^2+(x_2+1)^2 :  x_1 + 2x_2 = 1, 2x_1 + x_2 \geq 1 \} 
$$
In order to minimize this formulation in PEOpt, we use the code in Listing \ref{lst:simpleCon} and the input specification in Listing \ref{lst:simpleConJSON}, which generates the output in Listing \ref{lst:simpleConOut}.  As with the other examples in the introduction, we explain the functions used to generate this example in Chapter \ref{ch:Basic}, the input specification in Chapter \ref{ch:Input}, and the output in Chapter \ref{ch:Output}.

\input{simple_con_code.tex}
\input{simple_con_input.tex}
\input{simple_con_output.tex}

\chapter{Installation}\label{ch:Install}

    Depending on the requirements of the application, PEOpt has several different build an installation methods.  In the most simple case, PEOpt can be incorporated into a project as a header taken from the source directory.  In a more complete case, PEOpt uses CMake in order to manage the installation of several supporting libraries.  We discuss these methods below.

        Outside the installation mechanism, there are two primary modes in which we access the PEOpt routines: headers only and as a library.  In headers only mode, PEOpt enables access to the core optimization and verification routines, which include utilities such as finite-difference tests.  In library mode, PEOpt also includes predefined vector spaces as well as parameter readers that read in parameter files in JSON format.

\section{Header Only Installation with No Build System}

        At its core, PEOpt requires two headers: \url{peopt.h} and \url{linalg.h} found in the \url{./src/cpp/peopt} directory.  Copy these files to a directory labeled \url{peopt} in the incorporating projects source tree.  Then, add the
\begin{verbatim}
#include <peopt/peopt.h>
\end{verbatim}
directive to the source files where PEOpt is used.


\section{Header Only Installation with CMake}

        In order to do a header only installation with CMake, we require CMake version 2.4 or better.  In the base PEOpt directory, execute the following commands
\begin{verbatim}
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX=/dir/to/install/location ../src
make install
\end{verbatim}
This installs the appropriate PEOpt headers to the directory \url{/dir/to/install/location/include/peopt}.

\section{Library Installation with CMake}

        In order to fully install PEOpt using CMake, we require CMake version 2.8.9 or better.  Then, in the base PEOpt directory, execute the following commands
\begin{verbatim}
mkdir build
cd build
ccmake ../src
make
make install
\end{verbatim}
The command \texttt{ccmake} executes the curses version of CMake, which displays several different options.  The PEOpt specific flags are
\begin{center}
\begin{longtable}{lp{0.75\textwidth}}
Flag:         &\texttt{ENABLE\_DOCUMENTATION}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &None\\
Autodetect?:  &No\\
Description:  &Enables the build of the PEOpt manual from the LaTeX source.  It
              builds a pdf file of the manual.\\
\\
Flag:         &\texttt{ENABLE\_CPP\_LIBRARY}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{BLAS\_LIBRARY}, \texttt{LAPACK\_LIBRARY},
              \texttt{JSONCPP\_INCLUDE\_DIR}, \texttt{JSONCPP\_LIBRARY}\\
Autodetect?:  &No\\
Description:  &Enables the the library build for PEOpt.  This includes the
              built-in vector spaces for real vectors or cone programming.\\
\\
Flag:         &\texttt{ENABLE\_CPP\_EXAMPLES}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY}\\
Autodetect?:  &No\\
Description:  &Enables the build and installation of simple examples that
              demonstrate the use of PEOpt.  These examples are installed to
              the directory
              \url{${CMAKE_INSTALL_PREFIX}/share/peopt/cpp/examples}.\\
\\
Flag:         &\texttt{ENABLE\_CPP\_UNIT}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY}\\
Autodetect?:  &No\\
Description:  &Enables the build and installation of unit tests that help
              validate the PEOpt code.  These tests are installed to the
              directory \url{${CMAKE_INSTALL_PREFIX}/share/peopt/cpp/unit}.\\
\\
Flag:         &\texttt{ENABLE\_OPENMP}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY}\\
Autodetect?:  &No\\
Description:  &Toggles whether or not the PEOpt library should be compiled
              using OpenMP.  This primarily affect whether or not the
              prepackaged vector spaces are coded to run in parallel using
              threads.\\
\\
Flag:         &\texttt{ENABLE\_PYTHON}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY}, \texttt{PYTHON\_INCLUDE\_DIR}, \texttt{PYTHON\_LIBRARY}\\
Autodetect?:  &No\\
Description:  &Enables the build and installation of the Python wrappers for
              PEOpt.  The resulting Python module is installed to
              \url{${CMAKE_INSTALL_PREFIX}/share/peopt/python}.  As such, make
              sure that the PYTHONPATH includes this directory.  In addition,
              using this module assumes that libpeopt\_complete.so can be found
              in the LD\_LIBRARY\_PATH.\\
\\
Flag:         &\texttt{ENABLE\_PYTHON\_EXAMPLES}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{ENABLE\_PYTHON\_LIBRARY}\\
Autodetect?:  &No\\
Description:  &Enables the build and installation of examples that use the
              Python wrappers for PEOpt.  These examples are installed into
              the directory
              \url{${CMAKE_INSTALL_PREFIX}/share/peopt/python/examples.}\\
\\
Flag:         &\texttt{ENABLE\_MATLAB}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY}, \texttt{MATLAB\_INCLUDE\_DIR},
              \texttt{MATLAB\_LIBRARY}, \texttt{MATLAB\_MEX\_EXTENSION}\\
Autodetect?:  &No\\
Description:  &Enables the build and installation of the MATLAB wrappers for
              PEOpt.  The resulting MATLAB mex file is installed to
              \url{${CMAKE_INSTALL_PREFIX}/share/peopt/matlab}.  As such, make
              sure that the MATLAB path includes this directory. \\
\\
Flag:         &\texttt{ENABLE\_MATLAB\_EXAMPLES}\\
Type:         &Bool\\
Default:      &Off\\
Dependencies: &\texttt{ENABLE\_MATLAB\_LIBRARY}\\
Autodetect?:  &No\\
Description:  &Enables the build and installation of examples that use the
              MATLAB wrappers for PEOpt.  These examples are installed into
              the directory
              \url{${CMAKE_INSTALL_PREFIX}/share/peopt/matlab/examples}.\\
\\
Flag:         &\texttt{BLAS\_LIBRARY} \\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY} \\
Autodetect?:  &Yes \\
Description:  &A semicolon separated list of the complete path and library used
              to provide BLAS.  This must include all required libraries in
              order to successfully compile a BLAS dependent application.  For
              example, using ATLAS BLAS, one possible entry is:
              \url{/usr/lib/libf77blas.a;/usr/lib/libatlas.a}\\
\\
Flag:         &\texttt{LAPACK\_LIBRARY} \\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY} \\
Autodetect?:  &Yes \\
Description:  &A semicolon separated list of the complete path and library used
              to provide LAPACK.  This must include all required libraries,
              except for BLAS libraries above in \texttt{BLAS\_LIBRARY}, in
              order to successfully compile a LAPACK dependent application.
              For example, using ATLAS LAPACK, one possible entry is:
              \url{/usr/lib/liblapack.a;/usr/lib/libcblas.a;/usr/lib/libgfortran.so.3}\\
\\
Flag:         &\texttt{JSONCPP\_INCLUDE\_DIR} \\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY} \\
Autodetect?:  &Yes \\
Description:  &A path that indicates where the jsoncpp headers have been 
              installed.  The actual headers must be found in 
              \url{${JSONCPP_INCLUDE_DIR}/json/}.\\
\\
Flag:         &\texttt{JSONCPP\_LIBRARY}\\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_CPP\_LIBRARY}\\
Autodetect?:  &Yes \\
Description:  &The complete path and library for jsoncpp. \\
\\
Flag:         &\texttt{PYTHON\_INCLUDE\_DIR} \\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_PYTHON} \\
Autodetect?:  &Yes \\
Description:  &A path that indicates where the Python 2.7 headers have been 
              installed.  We do not prefix these headers, so we look directly
              in the directory provided here.\\
\\
Flag:         &\texttt{PYTHON\_LIBRARY}\\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_PYTHON}\\
Autodetect?:  &Yes \\
Description:  &The complete path and library for Python 2.7. \\
\\
Flag:         &\texttt{MATLAB\_INCLUDE\_DIR} \\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_MATLAB}\\
Autodetect?:  &Yes \\
Description:  &A path that indicates where the MATLAB headers have been 
              installed.  We do not prefix these headers, so we look directly
              in the directory provided here.\\
\\
Flag:         &\texttt{MATLAB\_LIBRARY}\\
Type:         &Path \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_MATLAB}\\
Autodetect?:  &Yes \\
Description:  &The complete path and library for the mex library.  Typically,
              this is libmex.so on Linux.\\
\\
Flag:         &\texttt{MATLAB\_MEX\_EXTENSION}\\
Type:         &String \\
Default:      &None \\
Dependencies: &\texttt{ENABLE\_MATLAB}\\
Autodetect?:  &No \\
Description:  &The extension of mex files on the system.  This can be found
              by typing in the command 'mexext' inside of MATLAB.  Note, copy
              the output from 'mexext' directly and do not add a '.'.
\end{longtable}
\end{center}
       
\chapter{Basic API}\label{ch:Basic}

       PEOpt organizes its algorithms into four different categories: unconstrained, inequality constrained, equality constrained, and constrained.  These categories correspond to the formulations
$$\begin{array}{l}
    \min\limits_{x\in X} f(x),\\
    \min\limits_{x\in X} \{ f(x) : h(x)\succeq 0\},\\
    \min\limits_{x\in X} \{ f(x) : g(x)=0 \},\\
    \min\limits_{x\in X} \{ f(x) : g(x)=0, h(x)\succeq 0 \},
\end{array}$$
respectively.  Since these formulations necessitate different algorithms, the overall structure of PEOpt and the algorithms themselves have been segregated into these categories.

        In order to interface to optimize the above formulations, we follow the general procedure
\begin{enumerate}
    \item Define the objective function.
    \item (Optional) Define the constraints.
    \item Define an initial guess for the optimization.
    \item (Optional) Allocate memory for elements in the codomain of the constraints.  These are the Lagrange multipliers. 
    \item Create an optimization state.
    \item Set the optimization parameters.
    \item Accumulate all the functions required for optimization into a single bundle.
    \item Call the appropriate optimization routine.
    \item Extract the solution.
\end{enumerate}
We discuss how to implement each of the above steps below. 

\section{Define the Objective Function}

        PEOpt defines the objective function as a class that inherits from\linebreak\texttt{peopt::ScalarValuedFunction}, which PEOpt defines as
\input{objectiveSpecification.tex}
This specification corresponds to the evaluation of the objective, $f(x)$, gradient, $\nabla f(x)$, and Hessian-vector product, $\nabla^2 f(x)$, respectively.  Strictly, PEOpt only requires $f(x)$ and $\nabla f(x)$, but an appropriate first order method such as SR1 must be chosen when setting the optimization parameters.  In the case where the Hessian is not available, set the result of the Hessian vector product to zero. 

        As an example, we define the Rosenbrock function as
$$
        f(x)=(1-x_1)^2+100(x_2-x_1^2)^2.
$$
This means we define the gradient as
$$
        \nabla f(x)=\begin{bmatrix}
        -400x_1(x_2-x_1^2)-2(1-x_1)\\
        200(x_2-x_1^2)
        \end{bmatrix}
$$
and Hessian-vector product as
$$
        \nabla^2 f(x)\delta x=
        \begin{bmatrix}
            1200x_1^2-400x_2+2 & -400x_1\\
            -400x_1 & 200
        \end{bmatrix}\delta x.
$$
Using PEOpt's internal vector spaces, we implement these operators as
\input{objectiveExample.tex}

\section{Define the Constraints}

        PEOpt defines the constraints as a vector-valued function that inherits from \texttt{peopt::VectorValuedFunction}, which PEOpt defines as
\input{constraintSpecification.tex}
This abstract class corresponds to the evaluation of the constraint, $g(x)$, Fr\'{e}chet derivative (Jacobian)-vector product, $g^\prime(x)\delta x$, adjoint of the F\'{e}chet derivative (Jacobian)-vector product, $g^\prime(x)^*\delta y$, and the second derivative adjoint-vector product, $(g^{\prime\prime}(x)\delta x)^*\delta y$.  Here, $^*$ denotes the Hilbert-adjoint, which is most often the transpose of the operator.  Alternatively, if we work in $\re^m$ with the $\ell^2$ inner product, we can define these operators as
\begin{align*}
        g(x)&=\begin{bmatrix}
            g_1(x)\\
            \dots\\
            g_m(x)
        \end{bmatrix}
        & g^\prime(x)\delta x&=\begin{bmatrix}
                \nabla g_1(x)^T \delta x\\
                \dots\\
                \nabla g_m(x)^T \delta x
        \end{bmatrix}\\
        g^\prime(x)^*\delta y&=
        \sum\limits_{i=1}^m \nabla g_i(x) [\delta y]_i 
        & (g^{\prime\prime}(x)\delta x)^*\delta y&=
        \sum\limits_{i=1}^m \nabla^2 g_i(x)\delta x [\delta y]_i 
\end{align*}
As with the objective function, the algorithms in PEOpt do not require second order information, but benefit from it when available.  As such, the evaluation of $(g^{\prime\prime}(x)\delta x)^*\delta y$ is not strictly required.  If not available, set the result of the second derivative adjoint-vector product to zero.

        For example, we define a simple vector-valued function as
$$
    f(x)=\begin{bmatrix}
        (x_1-2)^2 + (x_2-2)^2 - 1\\
        x_1 + 2x_2 - 1.
    \end{bmatrix}
$$
Then, we have that
\begin{align*}
    f^\prime(x)\delta x=&\begin{bmatrix}
        2(x_1-2) & 2(x_2-2)\\
        1 & 2
    \end{bmatrix}\begin{bmatrix}\delta x_1\\\delta x_2\end{bmatrix}\\
    f^\prime(x)^*\delta y=&
        \begin{bmatrix}
            2(x_1-2)\\
            2(x_2-2)
        \end{bmatrix}  \delta y_1
        +
        \begin{bmatrix}
            1\\
            2
        \end{bmatrix} \delta y_2\\
    (f^{\prime\prime}(x)\delta x)^*\delta y=&
        \begin{bmatrix}
            2 & 0\\
            0 & 2
        \end{bmatrix}\begin{bmatrix}\delta x_1\\\delta x_2\end{bmatrix}
            \delta y_1
        +\begin{bmatrix}
            0 & 0\\
            0 & 0
        \end{bmatrix}\begin{bmatrix}\delta x_1\\\delta x_2\end{bmatrix}
            \delta y_2.
\end{align*}
In code, we implement these operators as
\input{constraintExample.tex}

\section{Define an Initial Guess} 

        The class \texttt{peopt::Rm} used in each of our examples defines an algebra for vectors in $\re^m$.  Underneath, the storage used for these vectors is simply a \texttt{std::vector}.  As such, we define our initial guess using this class.

        As an example, our initial guess for the Rosenbrock function was $\begin{bmatrix}-1.2 & 1\end{bmatrix}^T$.  We implement this in code as
\input{initialGuessExample.tex}

\section{Allocate Memory for Elements in the Codomain of the Constraints}

        As with our initial guess, we also use the class \texttt{peopt::Rm} to represent elements in the codomain of our constraints.  These elements correspond to the Lagrange multipliers or dual variables for the constrained algorithms, but, beyond label, we require no additional information about duality theory.  In addition, these elements are purely used for allocating memory.  This means that the value of these vectors when passed to PEOpt is ignored.

        For example, if we have a single equality constraint, we allocate memory for the codomain as
\input{allocateCodomainExample.tex}

\section{Create an Optimization State}\label{sec:State}

        In PEOpt, the optimization state contains an entire description of the current state of the optimization algorithm.  This is unique to the particular optimization formulation, but all algorithms in a particular formulations share the same state.  Most algorithms do not require information about all of these pieces, but they are present to make it easier to switch from one algorithm to another.  For example, trust-region and line-search algorithms share several components, but the trust-region radius is unique to trust-region algorithms and the line-search step length is unique to line-search algorithms.  Nevertheless, we may want to switch from one algorithm to another, so they share the same components.

        In order to define an optimization state, we instantiate the state class within the particular class of formulation we require.  The syntax is
\input{stateSpecification.tex}
where these states correspond to the unconstrained, inequality constrained, equality constrained, and constrained formulations, respectively.  The variables \texttt{x}, \texttt{y}, and \texttt{z} correspond to elements in the domain of the objective, codomain of the equality constraints, and codomain of the inequality constraints, respectively.  Finally, the first template parameter denotes the floating point precision used by the optimization.  The remaining template parameters denote the vector space used by each of the elements, \texttt{x}, \texttt{y}, and \texttt{z}. 

\section{Set the Optimization Parameters}

        For each optimization problem, the parameters required for an efficient optimization solve can vary wildly.  Nevertheless, the parameters that guide this process reside within the state object.  There are two mechanisms for modifying these entries.  First, the state object created in Section \ref{sec:State} is simply a class with a variety of elements.  These can be modified directly.  Alternatively, if the PEOpt library is installed, we can use the JSON reader.  The syntax for reading a parameter file in JSON format from file is
\input{jsonReaderSpecification.tex}
where these four different commands correspond to unconstrained, inequality constrained, equality constrained, and constrained formulations, respectively.  As before, the template parameters denote first the precision required and then the vector spaces used by the optimization.

        Note, although the names of the parameters used by each are identical, the format is slightly different depending on whether we directly modify the state or use the JSON reader.  For example, the following JSON file modifies the stopping criteria for the gradient
\input{jsonReaderExample1.tex}
This has the same affect as the code
\input{jsonReaderExample2.tex}

        As far as the actual parameters and their affect, we discuss this in Chapter \ref{ch:Input}.

\section{Accumulate the Functions}

        In order to pass the functions used in the optimization to PEOpt, we accumulate each of them into a bundle of functions.  These bundles are simple structures that contain a number of \texttt{std::auto\_ptr} objects that point to the appropriate function.  The syntax for creating these objects is
\input{functionBundleSpecification.tex}
where \texttt{f} refers to a \texttt{ScalarValuedFunction} that denotes the objective, \texttt{g} refers to a \texttt{VectorValuedFunction} that denotes the equality constraint, and \texttt{h} refers to an affine \texttt{VectorValuedFunction} that denotes the inequality constraint.  Note, each of these structures only contains elements appropriate for their particular formulation.  For example, the bundle of functions that corresponds to unconstrained optimization only contains the element for \texttt{f} and not for \texttt{g} and \texttt{h}.

\section{Call the Optimization Solver}

       Once the state, parameters, and functions are set, calling the optimization solver is straightforward.  Simply, we call one of the four commands
\input{solverSpecification.tex}
These commands accept three input arguments: the messaging object, the bundle of functions, and the state.  The messaging object \texttt{peopt::Messaging()} sends the output to \texttt{stdout}.  This can be modified and we discuss how in Chapter \ref{ch:Advanced}.  Once the solver routine has been called, the optimization algorithms iterate until the stopping criteria is satisfied.  Generally, this creates a collection diagnostic information and we discuss the format of these output in Chapter \ref{ch:Output}.

\section{Extract the Solution}

        After the optimization routine concludes, the solution resides inside of the optimization state in a variable called \texttt{x}.  This is a \texttt{std::list} that contains a single element, the solution.  PEOpt stores each of its vector arguments inside of list in order to more efficiently manage memory.  This makes more advanced optimization techniques such as saving the state of the optimization and restarting a computation later easier.  The command required to extract the solution is the same for each formulation
\input{solutionExtractionSpecification.tex}

\chapter{Input Specifications}\label{ch:Input}

        The parameters that guide the optimization solver have a dramatic effect on the performance of the internal algorithms.  As such, we catalog these parameters and their affect below. 

\begin{longtable}{llp{0.4\textwidth}}
\multicolumn{3}{p{\textwidth}}{\bf Parameters for: Unconstrained, Inequality Constrained, Equality Constrained, Constrained}\\
Name & Default & Description\\
\texttt{eps\_grad} & $10^{-8}$ 
    & Relative tolerance for the gradient stopping criteria.  In unconstrained problems, PEOpt terminates when $\| \nabla f(x_k) \| \leq \epsilon \| \nabla f(x_0) \|$ where $x_k$ denotes the solution at the $k$-th iteration.  For inequality constrained problems, the gradient stopping criteria is $\| \ineqGradLag{x_k}{z_k} \| \leq \epsilon \| \ineqGradLag{x_0}{z_0}) \|$.  For equality constrained problems, this becomes $\| \eqGradLag{x_k}{y_k} \| \leq \epsilon \| \eqGradLag{x_0}{y_0} \|$.  Finally, for constrained problems, the stopping criteria is $\| \conGradLag{x_k}{y_k}{z_k} \| \leq \epsilon \| \conGradLag{x_0}{y_0}{z_0} \|$.\\
\\
\texttt{eps\_dx} & $10^{-8}$
    & Relative tolerance for the step length stopping criteria.  In unconstrained problems, PEOpt terminates when $\|\delta x\| \leq \epsilon \|\nabla f(x_0)\|$ where $\delta x$ denotes the step in the optimization variables at the current iteration and $x_0$ denotes the initial guess provided to the optimization.  For inequality constrained problems, PEOpt terminates when $\|\delta x\| \leq \epsilon \| \ineqGradLag{x_0}{z_0} \|$.  For equality constrained problems, this becomes $\|\delta x\| \leq \epsilon \| \eqGradLag{x_0}{y_0} \|$.  For constrained problems, this becomes $\|\delta x\| \leq \epsilon \| \conGradLag{x_0}{y_0}{z_0} \|$.\\
\\
\texttt{stored\_history} & 0
    & Number of vectors stored for use with quasi-Newton methods such as SR1 and BFGS\\
\\
\texttt{history\_reset} & 5
    & Number of failed trust-region iterations or line-search batches before the quasi-Newton information is discarded and PEOpt takes a steepest descent direction.\\
\\
\texttt{iter\_max} & 10
    & Maximum number of optimization iterations.\\
\\
\texttt{krylov\_iter\_max} & 10 
    & Maximum number of iterations taken by either truncated-CG or truncated-MINRES when solving the optimality system.\\
\\
\texttt{krylov\_orthog\_max} & 0 
    & Numbers of vectors stored and used in the orthogonalization of truncated-CG or truncated-MINRES.  In exact arithmetic, the number can be provably 0. In practice, if memory is available, it may be worthwhile to over orthogonalize. \\
\\
\texttt{eps\_krylov} & $10^{-2}$
    & Relative stopping criteria for truncated-CG or truncated-MINRES.  In truncated-CG, the stopping criteria when solving the system $Ax=b$ with preconditioner $B$ is $\|B(Ax_k-b)\|\leq \epsilon\|B(Ax_0-b)\|$.  In truncated-MINRES, the stopping criteria is $\|Ax_k-b\|_B \leq \epsilon\|Ax_0-b\|_B$.\\
\\
\texttt{krylov\_solver} & \texttt{ConjugateDirection}
    & Linear system solver used when solving the optimality conditions.  This can either be \texttt{ConjugateDirection} or \texttt{MINRES}.\\
\\
\texttt{algorithm\_class} & \texttt{TrustRegion}
    & Class of algorithm used in optimization.  This can either be \texttt{TrustRegion}, \texttt{LineSearch}, or \texttt{UserDefined}.\\
\\
\texttt{PH\_type} & \texttt{Identity}
    & Preconditioner used when solving the optimality conditions.  This can be either \texttt{Identity}, \texttt{InvBFGS}, or \texttt{InvSR1}, which correspond to the identity operator, a BFGS-based preconditioner, or a SR1 based preconditioner, respectively.\\
\texttt{H\_type} & \texttt{UserDefined}
    & Hessian approximation for the objective function.  This can be either \texttt{UserDefined}, \texttt{Identity}, \texttt{ScaledIdentity}, \texttt{BFGS}, or \texttt{SR1}.  If \texttt{UserDefined} is chosen, PEOpt uses the \texttt{hessvec} calculation inside of the objective function to calculate the Hessian-vector product.  Otherwise, the \texttt{Identity}, \texttt{BFGS}, and \texttt{SR1} approximations corresponds to the identity operator, BFGS, and SR1 operators, respectively.  Finally, the \texttt{ScaledIdentity} option directs PEOpt to use a scaled version of the identity matrix defined as $\|\nabla f(x)\|/(2\Delta) I$ where $\Delta$ denotes the trust-region radius.  This forces the algorithm to take a steepest-descent step the size of the trust-region at every iteration.  This implements a steepest-descent algorithm with a trust-region method. \\
\\
\texttt{msg\_level} & 1
    & Controls the amount of diagnostic output produced by PEOpt.  Here, 0 denotes no output and 1 denotes print information about each optimization iteration.\\
\\
\texttt{delta} & 1.0
    & Initial size of the trust-region radius\\
\\
\texttt{eta1} & 0.1
    & When the actual versus predicted reduction for a trust-region method is below this threshold, the step is rejected.  Otherwise, it is accepted.\\
\\
\texttt{eta2} & 0.9
    & When the actual versus predicted reduction for a trust-region method is above this threshold, the trust-region radius is increased when the trust-region radius inhibits the progress of the algorithm.\\
\\
\texttt{alpha0} & 1.0
    & Initial line-search step length.\\
\\
\texttt{linesearch\_iter\_max} & 5
    & Maximum number of line-search iterations conducted before checking for a reduction in the objective value.\\
\\
\texttt{dir} & \texttt{SteepestDescent}
    & Line-search direction taken by a line-search algorithm.  This can be either \texttt{SteepestDescent}, \texttt{FletcherReeves}, \texttt{PolakRibiere}, \texttt{HestenesStiefel}, \texttt{BFGS}, or \texttt{NewtonCG}.  The \texttt{FletcherReeves}, \texttt{PolakRibiere}, and \texttt{HestenesStiefel} directions corresponds to different flavors of Nonlinear-CG.  The \texttt{BFGS} direction implements a standard BFGS algorithm.  Finally, \texttt{NewtonCG} defines a Newton-CG algorithm that uses the current Hessian approximation defined by \texttt{H\_type} in order to calculate the direction.\\
\\
\texttt{kind} & \texttt{GoldenSection}
    & Kind of line-search algorithm used in a line-search algorithm.  This can be either \texttt{GoldenSection}, \texttt{BackTracking}, \texttt{TwoPointA}, or \texttt{TwoPointB}.  The algorithms \texttt{GoldenSection} and \texttt{BackTracking} correspond to a golden section search and a back tracking line-search, respectively.  \texttt{TwoPointA} and \texttt{TwoPointB} correspond to a line-search based on a two-point approximation of the Hessian.  This is used by the Barzilai-Borwein algorithm for unconstrained optimization.  If either \texttt{TwoPointA} or \texttt{TwoPointB} are chosen, it is important to set \texttt{dir} to \texttt{SteepestDescent}.\\
\\
\multicolumn{3}{p{\textwidth}}{\bf Parameters for: Inequality Constrained, Constrained}\\
Name & Default & Description\\
\texttt{eps\_mu} & $10^{-8}$
    & Absolute tolerance for the interior point stopping criteria.  PEOpt defines the estimate of the infeasibility with respect to the complementary slackness stopping condition as $\langle z,h(x) \rangle/\langle e,e\rangle$ where $z$ denotes the inequality multiplier and $e$ denotes the identity element for the inequality algebra.  For linear bound constraints, this is the vector of all ones, so $\langle e,e\rangle=m$ where $m$ denotes the number of variables.  Hence, the stopping criteria for the inequality constraints is $\mu < \epsilon$.  Since we always choose a starting inequality multiplier so that $\langle z,h(x) \rangle/\langle e,e\rangle=1$, this tolerance is effectively a relative tolerance as well.\\
\\
\texttt{sigma} & $0.5$
    & Rate at which PEOpt attempts to decrease the interior point parameter.  Specifically, if the current estimate of the interior point parameter is $\mu$, then we attempt to reduce $\mu$ to $\sigma \mu$.\\
\\
\texttt{gamma} & $0.95$
    & How close PEOpt allows a step in the optimization algorithm to approach the boundary.  A step of $1.0$ would allow a step to touch the boundary of the inequality constraint in a single step, which is disallowed by the interior point algorithm.\\
\\
\texttt{ipm} & \texttt{PrimalDual}
    & Kind of interior-point method used by PEOpt.  PEOpt allows either \texttt{PrimalDual}, \texttt{PrimalDualLinked}, or \texttt{LogBarrier}.  This corresponds to either a primal-dual interior point method, a primal-dual interior point method where the primal and dual step lengths are linked together, or a log-barrier algorithm.\\
\\
\texttt{cstrat} & \texttt{Constant}
    & Centrality strategy used by the interior-point method.  PEOpt allows either \texttt{Constant}, \texttt{StairStep}, or \texttt{PredictorCorrector} strategies.  This corresponds to a constant reduction in the interior point parameter by $\sigma$, reducing $\sigma$ only when the relative reduction in the interior point parameter $\mu$ is less than the relative reduction in the gradient stopping condition, or a predictor-corrector strategy where on even iterations $\sigma=0$ and on odd iterations $\sigma=1$.\\
\\
\multicolumn{3}{p{\textwidth}}{\bf Parameters for: Equality Constrained, Constrained}\\
Name & Default & Description\\
\texttt{zeta} & 0.8 & Fraction of the trust-region used for the quasi-normal step.\\
\\
\texttt{eta0} & 0.5 & Trust-region parameter that bounds the error in the predicted-reduction.\\
\\
\texttt{rho} & 1.0 & Initial penalty parameter used in the augmented Lagrangian.  For globalization purposes, the equality constrained algorithm uses an augmented Lagrangian of the form $f(x)+\langle y,g(x)\rangle+\rho\|g(x)\|^2$.  For the constrained algorithms, PEOpt uses an augmented Lagrangian of the form $f(x)+\langle y,g(x)\rangle+\rho\|g(x)\|^2-\mu \textnormal{barr}(h(x))$ where $\textnormal{barr}(x)$ denotes the barrier functioned defined in the inequality algebra.\\
\\
\texttt{rho\_bar} & $10^{-8}$ & Fixed increase in the penalty parameter in the augmented Lagrangian merit function.\\
\\
\texttt{eps\_constr} & $10^{-8}$ & Relative stopping tolerance for feasibility with respect to the equality constraint.  PEOpt requires that $\|g(x_k)\|<\epsilon\|g(x_0)||$ in order to terminate where $x_k$ denotes the solution at the $k$-th iteration.\\
\\
\texttt{xi\_all} & $10^{-4}$ & Relative stopping tolerance for all of the augmented system solves, \texttt{xi\_qn}, \texttt{xi\_pg}, \texttt{xi\_proj}, \texttt{xi\_proj}, \texttt{xi\_tang}, and \texttt{xi\_lmh}, described below.\\
\\
\texttt{xi\_qn} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the quasi-Newton step.\\
\\
\texttt{xi\_pg} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the projection of the gradient prior to solving the tangential subproblem.\\
\\
\texttt{xi\_proj} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the null-space projection of the iterate in the tangential subproblem.\\ 
\\
\texttt{xi\_tang} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the tangential step computation after solving the tangential subproblem. \\
\\
\texttt{xi\_lmh} & \texttt{xi\_all} & Relative stopping tolerance for the augmented system solve associated with the equality multiplier computation.\\
\\
\texttt{xi\_lmg} & $10^4$ & Absolute tolerance on the residual of the equality multiplier solve. \\
\\
\texttt{xi\_4} & 2.0 & Tolerance for how much error is acceptable after computing the tangential step given the result from the tangential subproblem.\\
\\
\texttt{augsys\_iter\_max} & 100 & Maximum number of GMRES iterations allowed when solving an augmented system.\\
\\
\texttt{augsys\_rst\_freq} & 0 & Restarts GMRES every specified number of iterations in order to save memory.  When 0, PEOpt uses no restarting.\\
\\
\texttt{PSchur\_left\_type} & \texttt{Identity} & Type of left Schur preconditioner used by PEOpt during an augmented system solve.  This can be either \texttt{Identity} or \texttt{UserDefined}.  An augmented system solve finds the solution of a linear system where the operator has the form $\begin{bmatrix} I & g^\prime(x)^*\\g^\prime(x) & 0\end{bmatrix}$.  The Schur preconditioner for this problem is an operator of the form $\begin{bmatrix} I & 0\\ 0 & B\end{bmatrix}$ where $B$ is the operator specified by this parameter.  Note, if we specify $B=(g^\prime(x)g^\prime(x)^*)^{-1}$, then the augmented system solves in 3 iterations.\\
\\
\texttt{PSchur\_right\_type} & \texttt{Identity} & Type of right Schur preconditioner used by PEOpt during an augmented system solve.  This can be either \texttt{Identity} or \texttt{UserDefined}.
\end{longtable}

\chapter{Output}\label{ch:Output}

        PEOpt generates a series of diagnostics while running that give information about the behavior and performance of the underlying algorithm.  This information is organized into columns that are exactly 10 characters wide.  Each column always has some sort of information, which makes the output easy to parse using standard Unix utilities such as \texttt{cut} or \texttt{awk}.  As far as the information in the columns themselves, we detail their meaning in the following table.

\begin{longtable}{lp{0.8\textwidth}}
\multicolumn{2}{p{\textwidth}}{\bf Output for: Unconstrained, Inequality Constrained, Equality Constrained, Constrained}\\
Name & Description\\
\texttt{Iter} & Current optimization iteration.  If the value of this entry is \texttt{*}, that means that a trust-region algorithm has rejected a step due to an unfavorable actual versus predicted reduction. \\
\\
\texttt{f(x)} & Value of the objective function at the start of the specified iteration.\\
\\
\texttt{merit(x)} & Value of the merit function at the start of the iteration.  For an unconstrained problem, this is simply $f(x)$.  For an inequality constrained problem, PEOpt returns $f(x)-\mu \textnormal{barr}(x)$ where $\textnormal{barr}(x)$ denotes the value of the barrier function from the inequality algebra.  For an equality constrained problem, PEOpt gives $f(x)-\langle y, g(x)\rangle + \rho \|g(x)\|^2$.  For a constrained problem, PEOpt defines this quantity as $f(x)-\langle y, g(x)\rangle + \rho \|g(x)\|^2 - \mu\textnormal{barr}(x)$.\\
\\
\texttt{||grad||} & Norm of the gradient of either the objective function or the Lagrangian.  In each case, this denotes the norm of the optimality conditions.  In the unconstrained case, PEOpt defines this quantity as $\|\nabla f(x)\|$.  In the inequality constrained case, this becomes $\|\ineqGradLag{x}{z}\|$.  In the equality constrained case, this is $\|\eqGradLag{x}{y}\|$.  In the fully constrained case, this quantity denotes $\|\conGradLag{x}{y}{z}\|$.\\\\
\texttt{||dx||} & Norm of the step taken during the last iteration. \\
\\
\texttt{ared} & Actual reduction defined as $\textnormal{merit}(x_k)-\textnormal{merit}(x_{k+1})$ where $x_k$ denotes the iterate at the previous step and $x_{k+1}$ denotes the current iterate.\\
\\
\texttt{pred} & Predicted reduction defined as $\textnormal{merit}(x_k)-\textnormal{model}(\delta x)$ where $x_k$ denotes the iterate at the previous step and $\delta x$ denotes the current step.  Depending on the formulation, PEOpt uses a different model, but these models generally correspond to a quadratic approximation to the merit function.\\
\\
\texttt{ared/pred} & Actual versus predicted reduction.  For a perfect model, this ratio is 1.0.\\
\\
\texttt{KryIter} & Number of iterations used by truncated-CG or MINRES when solving the optimality system.\\
\\
\texttt{KryErr} & Error in truncated-CG or MINRES when solving the optimality system.  For truncated-CG, this is defined as $\|B(Ax-b)\|$ where we are solving the system $Ax=b$ with the preconditioner $B$.  For truncated-MINRES, this becomes $\|Ax-b\|_B$.\\
\\
\texttt{KryWhy} & Why the truncated-CG or MINRES solve terminated.  This value can be either \texttt{RelErrSml}, the relative error is small, \texttt{NegCurv}, detected negative curvature, \texttt{IterExcd}, number of iterations exceeded, \texttt{TrstReg}, step violates the trust-region, \texttt{Unstable}, loss of orthogonality in the system, or \texttt{InvldCnt}, an invalid center for the trust-region radius.\\
\\
\texttt{LSIter} & Number of iterations taken by the line search.\\
\\
\multicolumn{2}{p{\textwidth}}{\bf Output for: Inequality Constrained, Constrained}\\
Name & Description\\
\texttt{mu} & Interior point parameter used by the optimization.  In other words, the target interior point parameter.\\
\\
\texttt{mu\_est} & Current estimate of the interior point parameter defined as $\langle z,h(x) \rangle/\langle e,e\rangle$ where $z$ denotes the inequality multiplier and $e$ denotes the identity element for the inequality algebra.  For linear bound constraints, this is the vector of all ones, so $\langle e,e\rangle=m$ where $m$ denotes the number of variables.\\
\\
\multicolumn{2}{p{\textwidth}}{\bf Output for: Equality Constrained, Constrained}\\
Name & Description\\
\texttt{||g(x)||} & Norm of the equality constraint at the start of the optimization iteration.  This denotes the infeasibility of the current iterate with respect to the equality constraints. 
\end{longtable}

\chapter{Advanced API}\label{ch:Advanced}

        PEOpt contains many additional features such as customizing the output and defining custom vector spaces.  We detail these features below.

\section{Messaging Object}

        PEOpt messaging objects inherit from \texttt{peopt::Messaging}, which PEOpt defines as
\input{messagingSpecification.tex}
Hence, messaging objects have two functions, \texttt{print} and \texttt{error}, that both accept a string and print the result.  The only difference between \texttt{print} and \texttt{error} is that \texttt{error} must safely exit the program.  Within a larger program, it is common to modify the messaging object so that PEOpt's output goes directly to a specified file.

\section{Custom Vector Spaces}

        PEOpt allows the use of a custom vector space provided by the user.  This allows PEOpt to more easily work with programs that store their information in a custom format.  Simply, we define a new vector space rather than trying to convert the internal data into a \texttt{std::vector}.  The requirements for a new vector space are a structure of the form
\input{vectorSpaceSpecification.tex}
Note, the functions \texttt{prod}, \texttt{id}, \texttt{linv}, \texttt{barr}, \texttt{srch}, and \texttt{symm} are only required for the codomain of the inequality constraint.  We explain these functions below.

\subsection{typedef}

       PEOpt requires that the custom vector space contains a typedef called Vector.  The underlying storage for the vector can be any valid class as long as the class contains a default constructor that does nothing and a copy constructor that knows how to copy items created by the default constructor.  As far as allocating memory for this structure, this is does with the \texttt{init} command that we describe below.

\subsection{init}

        Since allocating memory for a custom object may be complicated, PEOpt uses an \texttt{init} function that copies the structure of a vector from an already existing object.  Hence, this function can be seen as a deep copy from x to y.

\subsection{copy}
        
        The \texttt{copy} function copies the data from x to y using already initialized vectors.  Hence, it implements a shallow copy.

\subsection{scal}

        The \text{scal} function computes and returns the scalar multiple of a vector.  The scalar argument has the same type as the template argument to the structure.

\subsection{zero}

        The \texttt{zero} function transforms a vector into the zero element for the vector space.  In other words, the additive identity element.  The reason that \texttt{zero} exists separately from \texttt{scal} is that if a vector contains NaN elements, the multiplication of zero by NaN is undefined.

\subsection{axpy}

        Next, the function \texttt{axpy} computes the operator $\alpha x + y$ and then stores the result in $y$.

\subsection{innr}

        The inner product function \texttt{innr}, computes the inner product between two elements and then returns the result.  Recall, the inner product is a function $\langle \cdot,\cdot \rangle\rightarrow \re$ so that
\begin{enumerate}
    \item $\langle x,y\rangle = \langle y,x \rangle$,
    \item $\langle \alpha x,y\rangle = \alpha \langle x,y\rangle$,
    \item $\langle x+y,z\rangle = \langle x,z\rangle + \langle y,z\rangle$,
    \item $\langle x,x\rangle\geq 0$ with equality only when $x=0$.
\end{enumerate}
Note, PEOpt restricts the formulations to real vector spaces.  Note, for $\ell^2$, the inner product can be defined simply as $\langle x,y\rangle=x^Ty$.

\subsection{prod}

        The function \texttt{prod} defines a pseudo-Jordan product between two elements.  We say pseudo-Jordan in the sense that we do not require a full Euclidean-Jordan algebra.  Instead, we drop the requirement for commutativity.  Hence, for linear bound constraints, we define that
$$
        [x\circ y]_i = x_iy_i.
$$
Hence, the product denotes the pointwise or Hadamard product.  For second-order cone constraints, we define that
$$
        \begin{bmatrix}x_0\\\bar{x}\end{bmatrix} \circ \begin{bmatrix}y_0\\\bar{y}\end{bmatrix}=\begin{bmatrix} x_0y_0 + \bar{x}^T\bar{y}\\x_0 \bar{y} + y_0 \bar{x}\end{bmatrix}.
$$
For semidefinite programming, we have that
$$
        X\circ Y = XY.
$$
Alternatively, we can define that
$$
        X\circ Y = \frac{XY + YX}{2},
$$
but the inverse operation we discuss below will be far more inefficient.

\subsection{id} 

       The function \texttt{id} transforms a vector into the identity element for the inequality algebra.  Hence, it returns an element $e$ so that $x\circ e=x$.   For linear bound constraints, $e$ denotes the vector of all ones.  For second-order cone constraints, $e=\begin{bmatrix} 1 & 0 & \dots & 0\end{bmatrix}^T$.  For semidefinite constraints, $e=I$.

\subsection{linv}

        The function \texttt{linv} denotes the inverse operation to \texttt{prod}.  Note, $\texttt{prod}$ defines a bilinear operation so that there exists a linear operator $L(x)$ such that $x\circ y=L(x)y$.  The function \texttt{linv} computes the action of the {\it inverse} of $L(x)$ on a vector.  For linear bound constraints, $L(x)=\textnormal{Diag}(x)$, where $\textnormal{Diag}(x)$ denotes the diagonal matrix with $x$ on the diagonal.  For second-order cone constraints, $L(x)=\textnormal{Arw}(x)$ where we define $\textnormal{Arw}(x)$ as
$$
        \textnormal{Arw}\left(\begin{bmatrix}x_0\\\bar{x}\end{bmatrix}\right) =
\begin{bmatrix}
        x_0 & \bar{x}^T\\\bar{x} & x_0 I
\end{bmatrix}.
$$
For semidefinite constraints, we can either define that $L(X)=X$ or that $L(X)=\frac{X\cdot + \cdot X}{2}$.  Generally, it is preferable to use the first definition since $L(X)^{-1}=X^{-1}$.  In the second case, PEOpt requires the solution of the Sylvester equations.

\subsection{barr}

        The function \texttt{barr} denotes the barrier function used by the merit function.  PEOpt requires a function so that
$$
    x\circ \nabla \textnormal{barr}(x) = e.
$$
For linear bound constraints, this is simply the log-barrier function
$$
    \textnormal{barr}(x) = \sum\limits_{i=1}^m \log(x_i).
$$
For second-order cone constraints, we define this as
$$
    \textnormal{barr}\left(\begin{bmatrix}x_0\\\bar{x}\end{bmatrix}\right) = \frac{1}{2} \log(x_0^2-\langle \bar{x},\bar{x}\rangle).
$$
For semidefinite constraints, we define this as
$$
    \textnormal{barr}(X)=\log(\textnormal{det}(X))
$$
where $\textnormal{det}(X)$ denotes the determinant of $X$.

\subsection{srch}

        The function \texttt{srch} denotes the search function used to maintain strict feasibility with respect to the inequality constraint.  We define this as
$$
    \textnormal{srch}(x,y)=\arg\max\limits_{\alpha\geq 0} \{ \alpha\in\re : \alpha x + y \succeq 0\}
$$
where we assume $y\succ 0$.  Hence, $\alpha$ denotes the maximum distance PEOpt can move in the direction $x$ from $y$ so that $\alpha x + y$ remains feasible.

\subsection{symm}

        The function \texttt{symm} denotes the symmetrization operator.  PEOpt requires this operator since we relax the commutativity requirement from the Euclidean-Jordan algebra.  For linear bound constraints and second-order cone constraints, this operation does nothing.  In addition, for semidefinite constraints where $X\circ Y=\frac{XY+YX}{2}$, this operation does nothing.  However, for semidefinite constraints where $X\circ Y=XY$, PEOpt requires $\textnormal{symm}(X)=\frac{X+X^T}{2}$.

\end{document}
